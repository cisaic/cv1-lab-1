{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"https://d3i71xaburhd42.cloudfront.net/261c3e30bae8b8bdc83541ffa9331b52fcf015e6/3-Figure2-1.png\" width=50% > </center>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<center> \n",
    "    <font size=\"6\">Lab 1: Colour, Intrinsic Image Decomposition & Photometric Stereo</font> \n",
    "</center>\n",
    "<center> \n",
    "    <font size=\"4\">Computer Vision 1 University of Amsterdam</font> \n",
    "</center>\n",
    "<center> \n",
    "    <font size=\"4\">Due 23:59, September 13, 2024 (Amsterdam time)</font> \n",
    "</center>\n",
    "<center> \n",
    "    <font size=\"4\"><b>TA's: Xiaoyan, Floris, Adrian</b></font>\n",
    "</center>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "***\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<center>\n",
    "\n",
    "Student1 ID:  15558169 \\\n",
    "Student1 Name: Pradyut Nair\n",
    "\n",
    "Student2 ID: 15050025 \\\n",
    "Student2 Name: Christina Isaicu\n",
    "\n",
    "Student3 ID: 12708178\\\n",
    "Student3 Name: Akshay Sardjoe Missier\n",
    "\n",
    "Student4 ID: 13853163 \\\n",
    "Student4 Name: Benjamin Shaffrey\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:31.125797Z",
     "start_time": "2024-09-13T15:16:31.122874Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if sys.version_info[0] < 3:\n",
    "    raise Exception(\"Python 3 or a more recent version is required.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:31.791650Z",
     "start_time": "2024-09-13T15:16:31.137771Z"
    }
   },
   "outputs": [],
   "source": [
    "# environment and libraries\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action = \"ignore\", category = RuntimeWarning)\n",
    "warnings.simplefilter(action = \"ignore\", category = UserWarning)\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:31.826389Z",
     "start_time": "2024-09-13T15:16:31.823290Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make sure you're using the provided environment!\n",
    "assert cv2.__version__ == \"4.10.0\", \"You're not using the provided Python environment!\"\n",
    "assert np.__version__ == \"1.26.4\", \"You're not using the provided Python environment!\"\n",
    "assert matplotlib.__version__ == \"3.9.2\", \"You're not using the provided Python environment!\"\n",
    "# Proceed to the next cell if you don't get any error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Instructions**\n",
    "\n",
    "Your code and discussion must be handed in this jupyter notebook, renamed to **StudentID1_StudentID2_StudentID3.ipynb** before the deadline by submitting it to the Canvas Lab 1 Assignment. Please also fill out your names and ID's above.\n",
    "\n",
    "For full credit, make sure your notebook follows these guidelines:\n",
    "- It is mandatory to **use the Python environment provided** with the assignment; the environment specifies the package versions that have to be used to prevent the use of particular functions. Using different packages versions may lead to grade deduction. In the Python cell above you can check whether your environment is set up correctly.\n",
    "- To install the environment with the right package versions, use the following command in your terminal: ```conda env create --file=cv1_environment.yaml```, then activate the environment using the command ```conda activate cv1```.\n",
    "- Do not use additional packages or materials that have not been provided or explicitly mentioned.\n",
    "- Please express your thoughts **concisely**. The number of words does not necessarily correlate with how well you understand the concepts.\n",
    "- Answer all given questions and sub-questions.\n",
    "- Try to understand the problem as much as you can. When answering a question, give evidences (qualitative and/or quantitative results, references to papers, figures etc.) to support your arguments. Note that not everything might be explicitly asked for and you are expected to think about what might strengthen you arguments and make the notebook self-contained and complete.\n",
    "- Tables and figures must be accompanied by a **brief** description. Do not forget to add a number, a title, and if applicable name and unit of variables in a table, name and unit of axes and legends in a figure.\n",
    "\n",
    "__Note:__ A more complete overview of the lab requirements can be found in the Course Manual on Canvas\n",
    "\n",
    "Late submissions are not allowed. Assignments that are submitted after the strict deadline will not be graded. In case of submission conflicts, TAs’ system clock is taken as reference. We strongly recommend submitting well in advance, to avoid last minute system failure issues.\n",
    "\n",
    "Plagiarism note: Keep in mind that plagiarism (submitted materials which are not your work) is a serious crime and any misconduct shall be punished with the university regulations. This includes the use of generative tools such as ChatGPT.\n",
    "\n",
    "**ENSURE THAT YOU SAVE ALL RESULTS / ANSWERS ON THE QUESTIONS (EVEN IF YOU RE-USE SOME CODE).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview**\n",
    "\n",
    "- [Section 1: Colour Spaces (13 points)](#section-1)\n",
    "  - [Question 1 (2 points)](#question-1)\n",
    "  - [Question 2 (6 points)](#question-2)\n",
    "  - [Question 3 (2 points)](#question-3)\n",
    "  - [Question 4 (2 points)](#question-4)\n",
    "  - [Question 5 (1 point)](#question-5)\n",
    "- [Section 2: Colour Constancy (13 points)](#section-2)\n",
    "  - [Question 6 (8 points)](#question-6)\n",
    "  - [Question 7 (2 points)](#question-7)\n",
    "  - [Question 8 (3 points)](#question-8)\n",
    "- [Section 3: Intrinsic Image Decomposition (14 points)](#section-3)\n",
    "  - [Question 9 (2 points)](#question-9)\n",
    "  - [Question 10 (2 points)](#question-10)\n",
    "  - [Question 11 (4 points)](#question-11)\n",
    "  - [Question 12 (2 points)](#question-12)\n",
    "  - [Question 13 (2 points)](#question-13)\n",
    "  - [Question 14 (2 points)](#question-14)\n",
    "- [Section 4: Photometric Stereo (60 points)](#section-4)\n",
    "  - [Question 15 (4 points)](#question-15)\n",
    "  - [Question 16 (2 points)](#question-16)\n",
    "  - [Question 17 (1 point)](#question-17)\n",
    "  - [Question 18 (2 points)](#question-18)\n",
    "  - [Question 19 (2 points)](#question-19)\n",
    "  - [Question 20 (5 points)](#question-20)\n",
    "  - [Question 21 (3 points)](#question-21)\n",
    "  - [Question 22 (6 points)](#question-22)\n",
    "  - [Question 23 (2 points)](#question-23)\n",
    "  - [Question 24 (3 points)](#question-24)\n",
    "  - [Question 25 (3 points)](#question-25)\n",
    "  - [Question 26 (2 points)](#question-26)\n",
    "  - [Question 27 (4 points)](#question-27)\n",
    "  - [Question 28 (5 points)](#question-28)\n",
    "  - [Question 29 (2 points)](#question-29)\n",
    "  - [Question 30 (2 points)](#question-30)\n",
    "  - [Question 31 (4 points)](#question-31)\n",
    "  - [Question 32 (1 point)](#question-32)\n",
    "  - [Question 33 (2 points)](#question-33)\n",
    "  - [Question 34 (2 points)](#question-34)\n",
    "  - [Question 35 (1 point)](#question-35)\n",
    "  - [Question 36 (4 points)](#question-36)\n",
    "  - [Question 37 (1 point)](#question-37)\n",
    "- [Section X: Individual Contribution Report (Mandatory)](#section-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-1\"></a>\n",
    "### **Section 1: Colour Spaces**\n",
    "\n",
    "In this part of the assignment, you will study the different colour spaces for image representations and experiment how to convert a given RGB image to a specific colour space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-1\"></a>\n",
    "#### <font color='#FF0000'>Question 1 (2 points)</font>\n",
    "\n",
    "Why do we use the RGB color model as the basis for our digital cameras and photography? How does a standard digital camera capture a full RGB color image?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why**: \\\n",
    "The RGB colour model is created by superimposing light of three primary colours: Red, Green, and Blue. It is the most widely used colour model in digital cameras and photography due to its standardized method of defining all colours using the combination of these three primary colours, and the intensity of each colour based on additive colour mixing. It also provides a large range of colors and allows for colour consistency across different devices and platforms.\n",
    "\n",
    "**How**: \\\n",
    "A standard digital camera captures light in this way by using a sensor array that is made up of three types of photodetectors (photodiodes or pixels), each sensitive to one of the RGB primary colors. When light passes through the camera's lens, it is separated into the RGB components via a Bayer filter. When light strikes the sensor, each photodetector generates an electrical signal corresponding to the intensity of the light in its respective color channel. These signals are then processed by combining the RGB data from each pixel to create a full color image. It also interpolates the missing color information from surrounding pixels to form the complete image.\n",
    "\n",
    "\n",
    "Sources: https://proedu.com/blogs/photography-fundamentals/understanding-srgb-the-standard-color-space-for-photography, https://www.kentfaith.co.uk/blog/article_how-does-a-digital-camera-use-rgb_1397\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-2\"></a>\n",
    "#### <font color='#FF0000'>Question 2 (6 points)</font>\n",
    "\n",
    "Create a function to convert an RGB image into various color spaces using the provided `convert_colour_space()` function and other sub-functions.\n",
    "\n",
    "**Color Spaces to Convert:**\n",
    "\n",
    "1. **Grayscale**\n",
    "\n",
    "   Convert the RGB image into grayscale using 3 different methods mentioned in [John D. Cook's blog](https://www.johndcook.com/blog/2009/08/24/algorithms-convert-color-grayscale/). Additionally, check and report which method OpenCV uses for grayscale conversion, include it as well, and visualize all 4 methods in the same figure.\n",
    "\n",
    "2. **Opponent Color Space**\n",
    "\n",
    "   $\\begin{pmatrix}\n",
    "   O_1 \\\\\n",
    "   O_2 \\\\\n",
    "   O_3\n",
    "   \\end{pmatrix}$ = $\\begin{pmatrix}\n",
    "   \\frac{R-G}{\\sqrt{2}} \\\\\n",
    "   \\frac{R+G-2B}{\\sqrt{6}} \\\\\n",
    "   \\frac{R+G+B}{\\sqrt{3}}\n",
    "   \\end{pmatrix}$\n",
    "\n",
    "3. **Normalized RGB (rgb) Color Space**\n",
    "\n",
    "   $\\begin{pmatrix}\n",
    "   r \\\\\n",
    "   g \\\\\n",
    "   b\n",
    "   \\end{pmatrix}$ = $\\begin{pmatrix}\n",
    "   \\frac{R}{R+G+B} \\\\\n",
    "   \\frac{G}{R+G+B} \\\\\n",
    "   \\frac{B}{R+G+B}\n",
    "   \\end{pmatrix}$\n",
    "\n",
    "4. **HSV Color Space**\n",
    "\n",
    "   Convert the RGB image into HSV Color Space using OpenCV’s built-in function *cv2.cvtColor(img, cv2.RGB2HSV)*.\n",
    "\n",
    "5. **YCbCr Color Space**\n",
    "\n",
    "   Convert the RGB image into YCbCr Color Space using OpenCV’s built-in function *cv2.cvtColor(img, cv2.RGB2YCrCb)*. Arrange the channels in $Y, C_b,$ and $C_r$ order.\n",
    "\n",
    "**Note:** \n",
    "\n",
    "- Ensure you understand the data types and ranges required by the Python conversion and image displaying functions. This usually means [0, 1] for float datatype or [0, 255] for integer datatype. Explicitly change the datatype if necessary.\n",
    "\n",
    "- Each color space may have its own visualization requirements. For instance, think about how to meaningfully visualize the H, S, and V channels. Ensure each channel is visualized in an RGB manner where applicable.\n",
    "\n",
    "**Extra:** It might be interesting to visualize the image in its original color space (RGB) as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:31.834368Z",
     "start_time": "2024-09-13T15:16:31.831867Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_rgb_to_grays(input_image):\n",
    "    '''\n",
    "    Converts an RGB image into grayscale using different methods and \n",
    "    returns the grayscale images stacked as separate channels.\n",
    "\n",
    "    Args:\n",
    "        input_image: RGB image\n",
    "\n",
    "    Returns:\n",
    "        new_image: image with channels representing different grayscale conversion methods\n",
    "    '''\n",
    "    image_array = np.array(input_image)\n",
    "\n",
    "    # lightness method\n",
    "    lightness = (np.max(image_array, axis=-1) + np.min(image_array, axis=-1)) / 2\n",
    "\n",
    "    # average method\n",
    "    average = np.mean(image_array, axis=-1)\n",
    "\n",
    "    # luminosity method\n",
    "    luminosity = np.dot(image_array[..., :3], [0.21, 0.72, 0.07])\n",
    "\n",
    "    # built-in OpenCV function\n",
    "    bgr_image = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)\n",
    "    opencv = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # stack the results to easily visualize the 4 different methods\n",
    "    new_image = np.stack([lightness, average, luminosity, opencv], axis=-1)\n",
    "\n",
    "    # Normalize the image to the range [0, 255]\n",
    "    new_image = new_image.astype(np.uint8)\n",
    "\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:31.840327Z",
     "start_time": "2024-09-13T15:16:31.838311Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_rgb_to_opponent(input_image):\n",
    "    '''\n",
    "    Converts an RGB image into the opponent colour space and \n",
    "    returns the image with opponent colour channels.\n",
    "\n",
    "    Args:\n",
    "        input_image: RGB image (assumed to be in float32 format with range [0, 1])\n",
    "\n",
    "    Returns:\n",
    "        converted_image: image with channels representing opponent colour space (uint8, range [0, 255])\n",
    "    '''\n",
    "    input_image = np.array(input_image / 255) \n",
    "    red_channel = input_image[:, :, 0]\n",
    "    green_channel = input_image[:, :, 1]\n",
    "    blue_channel = input_image[:, :, 2]\n",
    "\n",
    "    opponent1 = (red_channel - green_channel) / np.sqrt(2)\n",
    "    opponent2 = (red_channel + green_channel - 2 * blue_channel) / np.sqrt(6)\n",
    "    opponent3 = (red_channel + green_channel + blue_channel) / np.sqrt(3)\n",
    "\n",
    "    new_image = np.stack([opponent1, opponent2, opponent3], axis=-1)\n",
    "\n",
    "   \n",
    "    # Convert to range [0, 255] and clip\n",
    "    new_image = (new_image * 255).astype(np.uint8)\n",
    "    #print(f\"Opponent: {new_image}\")\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:31.846677Z",
     "start_time": "2024-09-13T15:16:31.844494Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_rgb_to_normedrgb(input_image):\n",
    "    '''\n",
    "    Converts an RGB image into the normalized RGB (nrgb) colour space and \n",
    "    returns the image with normalized RGB channels.\n",
    "\n",
    "    Args:\n",
    "        input_image: RGB image\n",
    "\n",
    "    Returns:\n",
    "        new_image: image with channels representing normalized RGB colour space\n",
    "    '''\n",
    "    new_image = np.array(input_image)\n",
    "    red_channel = input_image[:, :, 0]\n",
    "    green_channel = input_image[:, :, 1]\n",
    "    blue_channel = input_image[:, :, 2]\n",
    "\n",
    "    # Avoid division by zero by adding a small epsilon to the denominator\n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    # Create a denominator to normalize the RGB channels\n",
    "    denominator = red_channel + green_channel + blue_channel + epsilon\n",
    "\n",
    "    # Normalize the RGB channels\n",
    "    normed_red = red_channel / denominator\n",
    "    normed_green = green_channel / denominator\n",
    "    normed_blue = blue_channel / denominator\n",
    "\n",
    "    # Stack the normalized channels to form the new image\n",
    "    new_image = np.stack([normed_red, normed_green, normed_blue], axis=-1)\n",
    "\n",
    "    #print(f\"Normalized RGB: {new_image}\")\n",
    "\n",
    "    # Normalize the image to the range [0, 255]\n",
    "    new_image = (new_image * 255).astype(np.uint8)\n",
    "\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:31.857099Z",
     "start_time": "2024-09-13T15:16:31.854771Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_colour_space(input_image, colourspace):\n",
    "    '''\n",
    "    Converts an RGB image into a specified colour space and \n",
    "    returns the image in its new colour space.\n",
    "\n",
    "    Args:\n",
    "        input_image: RGB image\n",
    "        colourspace: colour space to be converted to. \n",
    "        - choices: opponent, nrgb, hsv, ycbcr, grays\n",
    "\n",
    "    Returns:\n",
    "        new_image: image with channels in provided colour space\n",
    "    '''\n",
    "    # Convert the input image to float format\n",
    "    input_image = input_image.astype(np.float32)\n",
    "\n",
    "    if colourspace.lower() == 'opponent':\n",
    "        # fill in the rgb2opponent function\n",
    "        new_image = convert_rgb_to_opponent(input_image)\n",
    "\n",
    "    elif colourspace.lower() == 'nrgb':\n",
    "        # fill in the rgb2normedrgb function\n",
    "        new_image = convert_rgb_to_normedrgb(input_image)\n",
    "\n",
    "    elif colourspace.lower() == 'hsv':\n",
    "        # use built-in function from opencv\n",
    "        new_image = cv2.cvtColor(input_image, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "    elif colourspace.lower() == 'ycbcr':\n",
    "        # use built-in function from opencv\n",
    "        new_image = cv2.cvtColor(input_image/255, cv2.COLOR_RGB2YCrCb)\n",
    "        # Split the channels\n",
    "        y, cr, cb = cv2.split(new_image)\n",
    "        # Ensure the image is in the range [0, 255]\n",
    "        new_image = cv2.merge([y, cb, cr])\n",
    "        new_image = (new_image * 255).astype(np.uint8)\n",
    "\n",
    "    elif colourspace.lower() == 'grays':\n",
    "        # fill in the rgb2grays function\n",
    "        new_image = convert_rgb_to_grays(input_image)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError('Unknown colourspace type [%s]...' % colourspace)\n",
    "\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-3\"></a>\n",
    "#### <font colour='#FF0000'>Question 3 (2 points)</font>\n",
    "\n",
    "Visualize the image in each new colour space and its individual channels separately within the same figure. For example, for the HSV colour space, visualize the converted HSV image and its Hue, Saturation, and Value channels separately (4 images in 1 figure). Ensure that the channel visualizations are **meaningful**.\n",
    "\n",
    "The function `visualize_colourspace` below needs to be completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:31.871978Z",
     "start_time": "2024-09-13T15:16:31.866476Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_colourspace(image, colourspace='rgb'):\n",
    "    \"\"\"\n",
    "    Visualize the image in its corresponding colour space.\n",
    "    This function visualizes 1 figure with 4 images.\n",
    "    Each figure shows: \n",
    "    - Converted image in the new colour space.\n",
    "    - Its three (or more) separate channels visualized individually.\n",
    "\n",
    "    Args:\n",
    "        image: image with RGB channels (input)\n",
    "        colourspace: colour space to be converted to.\n",
    "        - choices: 'opponent', 'nrgb', 'hsv', 'ycbcr', 'grays'\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the image to the desired colour space\n",
    "    new_image = convert_colour_space(image, colourspace)\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    fig.suptitle(f\"{colourspace} colour Space\", fontsize=16)\n",
    "    \n",
    "    # Display the full converted image\n",
    "    axs[0, 0].imshow(new_image)\n",
    "    axs[0, 0].set_title(f\"{colourspace} Image\")\n",
    "    axs[0, 0].axis('off')\n",
    "    \n",
    "    # Handle different colour spaces and visualize their channels\n",
    "    if colourspace == 'opponent':\n",
    "        axs[0, 1].imshow(new_image[:, :, 0], cmap='gray')\n",
    "        axs[0, 1].set_title('Opponent Channel 1')\n",
    "        axs[0, 1].axis('off')\n",
    "        \n",
    "        axs[1, 0].imshow(new_image[:, :, 1], cmap='gray')\n",
    "        axs[1, 0].set_title('Opponent Channel 2')\n",
    "        axs[1, 0].axis('off')\n",
    "        \n",
    "        axs[1, 1].imshow(new_image[:, :, 2], cmap='gray')\n",
    "        axs[1, 1].set_title('Opponent Channel 3')\n",
    "        axs[1, 1].axis('off')\n",
    "\n",
    "    elif colourspace == 'nrgb':\n",
    "        axs[0, 1].imshow(new_image[:, :, 0], cmap='gray')\n",
    "        axs[0, 1].set_title('nRGB: Red Channel')\n",
    "        axs[0, 1].axis('off')\n",
    "\n",
    "        axs[1, 0].imshow(new_image[:, :, 2], cmap='gray')\n",
    "        axs[1, 0].set_title('nRGB: Green Channel')\n",
    "        axs[1, 0].axis('off')\n",
    "\n",
    "        axs[1, 1].imshow(new_image[:, :, 1], cmap='gray')\n",
    "        axs[1, 1].set_title('nRGB: Blue Channel')\n",
    "        axs[1, 1].axis('off')\n",
    "        \n",
    "\n",
    "    elif colourspace == 'hsv':\n",
    "        axs[0, 1].imshow(new_image[:, :, 0], cmap='hsv')  \n",
    "        axs[0, 1].set_title('HSV: Hue')\n",
    "        axs[0, 1].axis('off')\n",
    "        \n",
    "        axs[1, 0].imshow(new_image[:, :, 1], cmap='gray') \n",
    "        axs[1, 0].set_title('HSV: Saturation')\n",
    "        axs[1, 0].axis('off')\n",
    "        \n",
    "        axs[1, 1].imshow(new_image[:, :, 2], cmap='gray')  \n",
    "        axs[1, 1].set_title('HSV: Value')\n",
    "        axs[1, 1].axis('off')\n",
    "\n",
    "\n",
    "    elif colourspace == 'ycbcr':\n",
    "        axs[0, 1].imshow(new_image[:, :, 0], cmap='gray')\n",
    "        axs[0, 1].set_title('YCbCr: Y (Luma)')\n",
    "        axs[0, 1].axis('off')\n",
    "        \n",
    "        axs[1, 0].imshow(new_image[:, :, 1], cmap='gray')\n",
    "        axs[1, 0].set_title('YCbCr: Cb (Chroma)')\n",
    "        axs[1, 0].axis('off')\n",
    "        \n",
    "        axs[1, 1].imshow(new_image[:, :, 2], cmap='gray')\n",
    "        axs[1, 1].set_title('YCbCr: Cr (Chroma)')\n",
    "        axs[1, 1].axis('off')\n",
    "\n",
    "    elif colourspace == 'grays':\n",
    "        axs[0, 0].imshow(new_image[:, :, 0], cmap='gray')\n",
    "        axs[0, 0].set_title('Grayscale: Lightness')\n",
    "        axs[0, 0].axis('off')\n",
    "        \n",
    "        axs[0, 1].imshow(new_image[:, :, 1], cmap='gray')\n",
    "        axs[0, 1].set_title('Grayscale: Average')\n",
    "        axs[0, 1].axis('off')\n",
    "        \n",
    "        axs[1, 0].imshow(new_image[:, :, 2], cmap='gray')\n",
    "        axs[1, 0].set_title('Grayscale: Luminosity')\n",
    "        axs[1, 0].axis('off')\n",
    "        \n",
    "        axs[1, 1].imshow(new_image[:, :, 3], cmap='gray')\n",
    "        axs[1, 1].set_title('Grayscale: OpenCV')\n",
    "        axs[1, 1].axis('off')\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unknown colour space: {colourspace}\")\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:33.002279Z",
     "start_time": "2024-09-13T15:16:31.896120Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load and convert the image to RGB\n",
    "image = cv2.imread('images/beehive_house.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "visualize_colourspace(image, 'opponent')\n",
    "visualize_colourspace(image, 'nrgb')\n",
    "visualize_colourspace(image, 'hsv')\n",
    "visualize_colourspace(image, 'ycbcr')\n",
    "visualize_colourspace(image, 'grays')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-4\"></a>\n",
    "#### <font color='#FF0000'>Question 4 (2 points)</font>\n",
    "\n",
    "Explain each of the above 5 colour spaces and their properties. What are the benefits of using a different colour space other than RGB? Provide reasons for each of the above cases. You can include your observations from the visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Opponent Colour Space:\n",
    "- Properties: The opponent colour space transforms RGB values into three opponent colour channels. These are our interpretations of the channels-\n",
    "    - O_1: The difference between red and green channels. This encodes the red-green opponent contrast.\n",
    "    - O_2: A combination of red, green, and blue that encodes the blue-yellow contrast, because red and green create yellow due to their additive and subtractive properties.\n",
    "    - O_3: The sum of red, green, and blue, representing overall intensity or luminance.\n",
    "\n",
    "- Benefits: It retains the notions of hue and saturation presented in HSV, but adds a non-linear perceptual brightness. \n",
    "\n",
    "Source: https://graphics.stanford.edu/~boulos/papers/orgb_sig.pdf\n",
    "\n",
    "2. Normalized RGB (nRGB) Colour Space:\n",
    "- Properties: Normalized RGB (nRGB) colour space is based on the ratio of each channel to the sum of all channels. This space is useful for image analysis and manipulation tasks where the relative proportions of colour channels are important.\n",
    "- Benefits: It provides a normalized representation of colour, making it easier to analyse and compare images. This is particularly useful in scenarios where lighting variations or changes in viewing conditions affect the colour appearance of objects.\n",
    "\n",
    "3. HSV Colour Space:\n",
    "- Properties: HSV (Hue, Saturation, Value) is a cylindrical coordinate representation of points in an RGB colour space. Hue represents the colour type, Saturation represents the colour intensity, and Value represents the brightness.\n",
    "- Benefits: HSV is useful for colour-based segmentation and manipulation tasks. It separates colour information from intensity and brightness, making it easier to perform colour-related operations like colour quantization, colour-based image segmentation, and colour-based image retrieval.\n",
    "\n",
    "4. YCbCr Colour Space:\n",
    "- Properties: Y represents the luminance (grayscale intensity), and Cb and Cr represent the chrominance (colour information). YCbCr is used in video and digital photography.\n",
    "- Benefits: Since YCbCr represents colour using luminance and chrominance components, it is especially useful for colour video transmission as it reduces bandwidth without significantly impacting perceived image quality. This separation is also beneficial because the human eye is more sensitive to luminance than to chrominance, allowing for more efficient compression that aligns with human visual perception.\n",
    "\n",
    "Source: https://proedu.com/blogs/photography-fundamentals/ycbcr-colour-space-understanding-its-role-in-digital-photography#:~:text=Conversion%20from%20RGB%20to%20YCbCr,pivotal%20for%20bandwidth%2Drestricted%20applications.\n",
    "\n",
    "5. Grayscale Colour Space:\n",
    "- Properties: Grayscale images contain only intensity information, ranging from 0 (black) to 255 (white). Each type of grayscale method represents a different way to calculate this intensity.\n",
    "    - Lightness: Takes the average of highest and least contributing colour channels to represent intensity.\n",
    "    - Average: Takes the average of all channels to represent intensity.\n",
    "    - Luminosity: Takes the weighted sum of all channels to represent intensity.\n",
    "    - OpenCV: The image appears more balanced and natural in opencv's implementation of grayscale.\n",
    "- Benefits: It is simple and computationally efficient. Useful for practical applications where colour information is not crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-5\"></a>\n",
    "#### <font color='#FF0000'>Question 5 (1 point)</font>\n",
    "\n",
    "Find one more colour space from the literature, briefly explain its properties and give a use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another colourspace described in the literature (Computer Vision: Algorithms and Applications 2nd Edition, Richard Szeliski) is the CIELAB colour space.\n",
    "- Properties: CIELAB is a colour space that is designed to be perceptually uniform, meaning that it maintains a consistent colour difference for equal perceived colour differences. It separates colour information from intensity and brightness, making it easier to perform colour-based operations like colour quantization, colour-based image segmentation, and colour-based image retrieval.\n",
    "- Benefits: It allows for perception of relative differences in colour, which is useful for colour-based image processing tasks, such as image segmentation, image retrieval, and image compression.\n",
    "\n",
    "Sources: https://en.wikipedia.org/wiki/CIELAB_colour_space, Computer Vision: Algorithms and Applications 2nd Edition, Richard Szeliski\n",
    "<a id=\"section-2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-2\"></a>\n",
    "### **Section 2: Colour Constancy**\n",
    "\n",
    "Colour constancy is the ability to perceive the colors of objects consistently, regardless of the color of the light source. The goal of color constancy algorithms is to estimate the light source's illuminant and then correct the image so that it appears as if taken under a canonical (white) light source. In digital cameras, the automatic white balance (AWB) function performs this task to ensure images look natural.\n",
    "\n",
    "In this part of the assignment, you will implement the well-known Grey-World Algorithm, a fundamental color constancy algorithm. This algorithm operates under the assumption that, under white light, the average color in a scene should be grey ([128, 128, 128]).\n",
    "\n",
    "For more information, refer to the [Grey World algorithm on Wikipedia.](https://en.wikipedia.org/wiki/Color_normalization#Grey_world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-6\"></a>\n",
    "#### <font color='#FF0000'>Question 6 (8 points)</font>\n",
    "\n",
    "Complete the function `apply_grey_world_algorithm` to apply color correction to an RGB image using the Grey-World algorithm.\n",
    "\n",
    "Display the original image and the color-corrected one on the same figure. Use the `beehive_house.jpg` image to test your algorithm. You should see that the reddish color cast on the image is removed, making it look more natural.\n",
    "\n",
    "***Note:*** You do not need to apply any pre or post-processing steps. For the calculation or processing, you are not allowed to use any available code or any dedicated library function except *standard Numpy functions*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:33.287948Z",
     "start_time": "2024-09-13T15:16:33.285479Z"
    }
   },
   "outputs": [],
   "source": [
    "# Chris's code\n",
    "def apply_grey_world_algorithm(awb_img):\n",
    "    \"\"\"\n",
    "    Apply the Grey-World algorithm to correct the color balance of an RGB image.\n",
    "\n",
    "    This function assumes that under a white light source, the average color in a scene should be grey.\n",
    "    It corrects the image by scaling each channel so that the mean of each channel is equal.\n",
    "\n",
    "    Args:\n",
    "        awb_img: RGB image to be color corrected\n",
    "\n",
    "    Returns:\n",
    "        gw_img: Color corrected image using the Grey-World algorithm\n",
    "    \"\"\"\n",
    "    # calculate grey world algorithm\n",
    "    # scale each channel so that the mean of each channel is equal\n",
    "    \n",
    "    # copy to avoid modifying original image\n",
    "    # convert to float for normalization\n",
    "    \n",
    "    gw_img = np.zeros_like(awb_img, dtype=np.float32)\n",
    "    \n",
    "    grey = 128 # assume the image mean is grey value of 128\n",
    "    \n",
    "    # loop through all 3 colour channels\n",
    "    for i in range(0, awb_img.shape[-1]):\n",
    "        \n",
    "        # calculate the mean of the channel\n",
    "        channel_mean = np.mean(awb_img[:,:,i])\n",
    "        # print(channel_mean)\n",
    "        \n",
    "        # scale the channel by the ratio of the grey value to the channel mean\n",
    "        gw_img[:,:,i] = awb_img[:,:,i] * (grey / channel_mean)\n",
    "\n",
    "    # make sure img is between range 0 - 255 and is int format for rgb\n",
    "    return gw_img.clip(0, 255).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:33.541354Z",
     "start_time": "2024-09-13T15:16:33.452281Z"
    }
   },
   "outputs": [],
   "source": [
    "# load and convert the image to rgb\n",
    "image = cv2.imread('images/beehive_house.jpg')\n",
    "\n",
    "# image = cv2.imread('intrinsic_images/bed1_sl.png')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "gw_img = apply_grey_world_algorithm(image)\n",
    "\n",
    "# Show each separate RGB channel in subplots\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "\n",
    "# Red channel\n",
    "axes[0].imshow(image)\n",
    "axes[0].set_title('Base image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Green channel\n",
    "axes[1].imshow(gw_img)\n",
    "axes[1].set_title('Grey world normalized image')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Save the plot\n",
    "# fig.savefig('./images/Q6-grey-world.jpg', bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-7\"></a>\n",
    "#### <font color='#FF0000'>Question 7 (2 points)</font>\n",
    "\n",
    "Give an example case for Grey-World Algorithm on where it might fail. Include your reasoning.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Write your answer here.*\n",
    "\n",
    "The Grey-World algorithm doesn't work if an image is dominated by a single colour. This is because the algorithm assumes that the average colour of the image should be grey (i.e. there are many different colours in the image), but if that's not the case (ex. a picture of a blue sky), the algorithm would decrease the intensity of the dominant channel and increase the intensity of the other channels in ways that may look unnatural. \n",
    "\n",
    "The grey-world algorithm does not distinguish between the lighting of an image and the original colour of objects in the image. As a result, if the image is dominated by a single colour that is intrinsic to the objects in the image (ex. a close up of red apples, or a blue sky), the colour correction would result in unnatural hues in the other channels. \n",
    "\n",
    "The grey world algorithm could fail for pictures taken at night or in other dark areas. A nighttime picture should be quite dark, but this algorithm will brighten the color channels, distorting the true colors. \n",
    "\n",
    "The assumptions made by the algorithm also affect the results. If the assumption is that the average of every image is [128, 128, 128], results may differ to the assumption that the mean of an image is grey. \n",
    "\n",
    "Assumption 1: If the channel pixels are scaled by the ratio of the image mean to the channel mean, then the following holds: \n",
    "\n",
    "The algorithm would fail is if it were used in video-processing. The channel average would need to be re-calculated if a new object enters the scene which would not only be computationally expensive, it would also result in a different \"grey\" average for each frame, producing a flickering effect.\n",
    "\n",
    "Assumption 2: If the channel pixels are scaled by the ratio of 128 to the channel mean, the following holds: \n",
    "\n",
    "The grey world algorithm could fail for pictures taken at night or in other dark areas. A nighttime picture should be quite dark, but this algorithm will brighten the color channels (to get closer to the 128 value), thus distorting the true colors. \n",
    "\n",
    "Source: \n",
    "* [Grey World algorithm on Wikipedia.](https://en.wikipedia.org/wiki/Color_normalization#Grey_world)\n",
    "* Lee, D., & Plataniotis, K. N. (2013). A taxonomy of color constancy and invariance algorithm. In Lecture notes in computational vision and biomechanics (pp. 55–94). https://doi.org/10.1007/978-94-007-7584-8_3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-8\"></a>\n",
    "#### <font color='#FF0000'>Question 8 (3 points)</font>\n",
    "\n",
    "Find out one more colour constancy algorithm from the literature and explain it briefly.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Write your answer here.*\n",
    "Another colour constancy algorithm is the White Patch method which assumes that the brightest point in an image is pure white. The algorithm uses the lightest patch of the image as a reference point to normalize the image. \n",
    "> \n",
    "> Each channel is scaled by the maximum pixel value within that channel. This works well when there aren't any maximum value pixels (255) already in the image, but there are very clear bright areas. One solution to improve this algorithm is to use the pixel values at a specified percentile threshold (ex. 98th percentile), so the reference point is more representative of the image. \n",
    "> \n",
    "> The algorithm works best for images with clear bright areas, but can fail for images that violate the assumption of the White Patch method (i.e. the brightest point in the image is not white). The algorithm also does not work well if the image is not uniformly illuminated, or there are multiple light sources, because the normalization is done with respect to one assumed light source.\n",
    "> \n",
    "> Source:\n",
    "> Rizzi, A., Gatta, C. and Marini, D. (2002) ‘Color correction between Gray World and White Patch’, SPIE Proceedings. doi:10.1117/12.469534. \n",
    "> Ebner, M. (2021). Color constancy. In Computer Vision: A Reference Guide (pp. 168-175). Cham: Springer International Publishing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3\"></a>\n",
    "### **Section 3: Intrinsic Image Decomposition**\n",
    "\n",
    "Intrinsic image decomposition is the process of separating an image into its formation components, such as reflectance (albedo) and shading (illumination). <a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1) Then, under the assumptions of body (diffuse) reflection, linear sensor response and narrow band filters, the decomposition of the observed image $I(\\vec{x})$ at position $\\vec{x}$ can be approximated as the element-wise product of its albedo $R(\\vec{x})$ and shading $S(\\vec{x})$ intrinsics:\n",
    "\n",
    "$$I(\\vec{x})=R(\\vec{x}) \\times S(\\vec{x})$$\n",
    "\n",
    "In this part of the assignment, you will experiment with intrinsic image components to perform a particular computational photography application: material recolouring. For the experiments, we will use images from a synthetic intrinsic image dataset. <a name=\"cite_ref-2\"></a>[<sup>[2]</sup>](#cite_note-2)\n",
    "\n",
    "<a name=\"cite_note-1\"></a><small>[1.](#cite_ref-1) H. G. Barrow and J. M. Tenenbaum. Recovering intrinsic scene characteristics from images. Computer Vision Systems, pages 3-26, 1978.</small>\n",
    "\n",
    "<a name=\"cite_note-2\"></a><small>[2.](#cite_ref-1) http://www.cic.uab.cat/Datasets/synthetic_intrinsic_image_dataset/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-9\"></a>\n",
    "#### <font color='#FF0000'>Question 9 (2 points)</font>\n",
    "\n",
    "In what other components can an image be decomposed other than albedo and shading? Give an example and explain the concepts in your answer.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surface normals (shape from texture): One way an image can be decomposed is by extracting regular patters from the image and computing deformations in the pattern to recover the shape of the surface through surface normals or other methods. One example of such a regular pattern could be printed textured cloth. \n",
    "\n",
    "> Source: White, Ryan, Keenan Crane, and D. A Forsyth. “Capturing and Animating Occluded Cloth.” ACM transactions on graphics 26.3 (2007): n. pag. Web.\n",
    "> Jeon, J., Cho, S., Tong, X., Lee, S. (2014). Intrinsic Image Decomposition Using Structure-Texture Separation and Surface Normals. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds) Computer Vision – ECCV 2014. ECCV 2014. Lecture Notes in Computer Science, vol 8695. Springer, Cham. https://doi.org/10.1007/978-3-319-10584-0_15\n",
    "> CV textbook chapter 13.1.2\n",
    " \n",
    "Shape from focus: \n",
    "The depth of an image can be estimated from the blur that occurs when an object is moved away from the focus plane (towards or away from the camera).\n",
    "\n",
    "> CV textbook chapter 13.1.3\n",
    "\n",
    "Specular reflection: Along with diffuse (shading) decomposition, an image can also be decomposed into its highlights, meaning the light reflecting off a surface. \n",
    "\n",
    "> Source: CV textbook chapter 2.2 Photometric image formation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-10\"></a>\n",
    "#### <font color='#FF0000'>Question 10 (2 points)</font>\n",
    "\n",
    "If you check the literature, you will see that almost all intrinsic image decomposition datasets are composed of synthetic images. What might be the reason for that?\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Synthetic datasets allow for many different simulated light settings and scenes. These are also far easier to create than similar real world datasets. Additionally, ground truths are always available for synthetic datasets, whereas these might be hard to acquire for real world datasets.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-11\"></a>\n",
    "#### <font color='#FF0000'>Question 11 (4 points)</font>\n",
    "\n",
    "Choose an object with a single color from the `/intrinsic_images/` folder, which is based on the [synthetic intrinsic image dataset](http://www.cic.uab.cat/Datasets/synthetic_intrinsic_image_dataset/).\n",
    "\n",
    "Demonstrate that you can reconstruct the original PNG image from its intrinsics using the albedo and shading. Your script should output a figure displaying the original image, its intrinsic images, and the reconstructed one. Complete the function `reconstruct_image_from_intrinsics()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:33.668545Z",
     "start_time": "2024-09-13T15:16:33.665539Z"
    }
   },
   "outputs": [],
   "source": [
    "def reconstruct_image_from_intrinsics(albedo_img, shading_img):\n",
    "    \"\"\"\n",
    "    Reconstruct the original image from its intrinsic images (albedo and shading).\n",
    "\n",
    "    This function takes the albedo and shading images as input, \n",
    "    and reconstructs the original image by multiplying albedo with shading.\n",
    "\n",
    "    Args:\n",
    "        albedo_img: Image representing the albedo (reflectance)\n",
    "        shading_img: Image representing the shading\n",
    "\n",
    "    Returns:\n",
    "        iid_img: Reconstructed image\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reflectance * Shading\n",
    "    iid_img = (albedo_img / 255) * (shading_img/255)\n",
    "    iid_img = np.clip(iid_img * 255, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    return iid_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:33.862668Z",
     "start_time": "2024-09-13T15:16:33.698731Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replace the object name with a valid name from the dataset\n",
    "img_path = './intrinsic_images/'\n",
    "object_name = 'turt_ml1'\n",
    "\n",
    "# Read with opencv\n",
    "obj = cv2.imread(img_path + object_name + '.png')\n",
    "if obj is None:\n",
    "    raise FileNotFoundError(f'No image found with filename: {img_path + object_name + \".png\"}')\n",
    "else:\n",
    "    print(f'object shape: {obj.shape}')\n",
    "\n",
    "obj_sha = cv2.imread(img_path + object_name + '_shad.png')\n",
    "if obj_sha is None:\n",
    "    raise FileNotFoundError(f'No image found with filename: {img_path + object_name + \"_shad.png\"}')\n",
    "else:\n",
    "    print(f'object albedo shape: {obj_sha.shape}')\n",
    "\n",
    "obj_alb = cv2.imread(img_path + object_name.split('_')[0] + '_refl.png')\n",
    "if obj_alb is None:\n",
    "    raise FileNotFoundError(f'No image found with filename: {img_path + object_name.split(\"_\")[0] + \"_refl.png\"}')\n",
    "else:\n",
    "    print(f'object shading shape: {obj_alb.shape}')\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "recon = reconstruct_image_from_intrinsics(obj_alb, obj_sha)\n",
    "titles = [\"Original\", \"Albedo\", \"Shading\", \"Reconstruction\"]\n",
    "obj_imgs = [obj, obj_alb, obj_sha, recon]\n",
    "\n",
    "plt.figure(figsize=(18,8))\n",
    "for i,n in enumerate(titles):\n",
    "    plt.subplot(1, len(titles), i+1)\n",
    "    plt.imshow(cv2.cvtColor(obj_imgs[i], cv2.COLOR_BGR2RGB))\n",
    "    plt.title(n)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-12\"></a>\n",
    "#### <font color='#FF0000'>Question 12 (2 points)</font>\n",
    "\n",
    "Manipulating colors in photographs is an important problem with many applications in computer vision. Recoloring algorithms aim to manipulate colors effectively, and better results can be obtained if the albedo image is available as it is independent of confounding illumination effects.\n",
    "\n",
    "First determine the true material color of the object you picked in RGB space (uniform color in this case). \n",
    "\n",
    "- Complete the code for the function `get_true_color()`.\n",
    "- Plot the true color of the object in RGB space, make sure to also plot each channel separately. \n",
    "\n",
    "**Hint:** You can use `np.tile` to create a visualization of the true color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:34.034673Z",
     "start_time": "2024-09-13T15:16:34.032292Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_true_color(albedo_img):\n",
    "    \"\"\"\n",
    "    Determines the true material color of the object in the albedo image.\n",
    "\n",
    "    This function calculates the average RGB color in the albedo image, which represents the true material color of the object.\n",
    "\n",
    "    Args:\n",
    "        albedo_img: Albedo (reflectance) image\n",
    "\n",
    "    Returns:\n",
    "        true_colour: True material color of the object in RGB space\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    y,x,c = albedo_img.shape\n",
    "    #r,g,b = (np.mean(albedo_img[:,:,i]) for i in range(c))\n",
    "    r,g,b = ((np.sum(albedo_img[:,:,i]) / np.sum(albedo_img[:,:,i] > 0)) for i in range(c)) \n",
    "    true_color = np.ones_like(albedo_img)   \n",
    "    #true_color = np.tile(np.array([r,g,b]), (y,x,1))\n",
    "    for i,j in enumerate((r,g,b)):\n",
    "        true_color[:,:,i] *= int(j)\n",
    "\n",
    "    return true_color.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:34.178658Z",
     "start_time": "2024-09-13T15:16:34.093987Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the true color of the object's albedo image\n",
    "obj_alb_rgb = cv2.cvtColor(obj_alb, cv2.COLOR_BGR2RGB)\n",
    "true_color = get_true_color(obj_alb_rgb)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "plt.figure(figsize=(18,8))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(true_color)\n",
    "plt.title(\"True color\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "for i,c in enumerate([\"R\", \"G\", \"B\"]):\n",
    "    color_copy = np.zeros_like(true_color)\n",
    "    color_copy[:,:,i] = true_color[:,:,i]\n",
    "    plt.subplot(1, 4, i+2)\n",
    "    plt.imshow(color_copy)\n",
    "    plt.title(c)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-13\"></a>\n",
    "#### <font color='#FF0000'>Question 13 (2 points)</font>\n",
    "\n",
    "Assume you are given the PNG image and have access to its intrinsic albedo and shading images.\n",
    "\n",
    "Recolor the object's image with pure red (255, 0, 0). Complete the code for the function `recolor_image()`. Display the recolored version of the object. Make sure to also plot each channel separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:34.294478Z",
     "start_time": "2024-09-13T15:16:34.292158Z"
    }
   },
   "outputs": [],
   "source": [
    "def recolor_image(albedo_img, shading_img):\n",
    "    \"\"\"\n",
    "    Recolors an image based on its albedo and shading images.\n",
    "\n",
    "    This function recolors the input albedo image by changing non-black pixels to red\n",
    "    and then reconstructs the final image using the provided shading image.\n",
    "\n",
    "    Args:\n",
    "        albedo_img: Albedo (reflectance) image\n",
    "        shading_img: Shading image\n",
    "\n",
    "    Returns:\n",
    "        recolored_img: Recolored and reconstructed image\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    mask = np.sum(albedo_img, axis=2)\n",
    "    mask[mask > 0] = 255\n",
    "    n_albedo = np.zeros_like(albedo_img)\n",
    "    n_albedo[:,:,0] = mask\n",
    "    return reconstruct_image_from_intrinsics(n_albedo, shading_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:34.424738Z",
     "start_time": "2024-09-13T15:16:34.332390Z"
    }
   },
   "outputs": [],
   "source": [
    "recolored_image = recolor_image(obj_alb, obj_sha)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "plt.figure(figsize=(18,8))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(recolored_image)\n",
    "plt.title(\"Recolored\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "for i,c in enumerate([\"R\", \"G\", \"B\"]):\n",
    "    color_copy = np.zeros_like(recolored_image)\n",
    "    color_copy[:,:,i] = recolored_image[:,:,i]\n",
    "    plt.subplot(1, 4, i+2)\n",
    "    plt.imshow(color_copy)\n",
    "    plt.title(c)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-14\"></a>\n",
    "#### <font color='#FF0000'>Question 14 (2 points)</font>\n",
    "\n",
    "Although you have recoloured the object with pure red, the reconstructed images do not seem to display those pure colors and thus the colour distributions over the object do not appear uniform. Explain the reason.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*The shading image darkens parts of the object that are not directly illuminated.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__  This was a simple case where the image is synthetic, object centered and has only one colour, and you have access to its ground-truth intrinsic images. Real world scenarios require more than just replacing a single colour with another, not to mention the complexity of achieving a decent intrinsic image decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-4\"></a>\n",
    "### **Section 4: Photometric Stereo**\n",
    "\n",
    "In this section, we delve into the photometric stereo technique, a method used to reconstruct a surface's shape from a set of images captured under varying lighting conditions. This approach is based on the principles outlined in Section 5.4 of Forsyth and Ponce's *Computer Vision: A Modern Approach*, which provides a comprehensive introduction to the theory and application of photometric stereo in computer vision.\n",
    "\n",
    "Photometric stereo is particularly useful for capturing fine details of surface geometry. The core idea is to utilize an orthographic camera to take multiple images of a surface, each under different lighting conditions. By analyzing the variations in image intensity across these images, we can infer the surface normals and, subsequently, reconstruct the surface's height at each pixel, leading to what is commonly known as a height map or depth map.\n",
    "\n",
    "The method assumes that the camera and the surface remain stationary, with illumination coming from different directions. By capturing enough images with different light source vectors, we can solve for the surface normals and albedo at each pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-15\"></a>\n",
    "#### <font color='#FF0000'>Question 15 (4 points)</font>\n",
    "\n",
    "Start with the grayscale sphere model located in the SphereGray5 folder. The folder contains 5 images of a sphere with a grayscale checker texture under different lighting conditions. Your task is to estimate the surface reflectance (albedo) and surface normal of this model.\n",
    "\n",
    "In this question, you will need to complete the function `estimate_albedo_and_normals()` to estimate the albedo and surface normal map for the SphereGray5 folder.\n",
    "\n",
    "To assist you, two helper functions are provided:\n",
    "\n",
    "- `load_synthetic_images()`: This function loads the synthetic images from the specified directory.\n",
    "- `show_results()`: This function displays the albedo, surface normals, and optionally the height map and surface error.\n",
    "\n",
    "These helper functions are provided to simplify the process of loading images and displaying your results.\n",
    "\n",
    "**Hint**: To get the least-squares solution of a linear system, you can use the `numpy.linalg.lstsq` function.\n",
    "\n",
    "Make sure to include images of your results in your notebook at key points. When visualizing 3D models, choose viewpoints that clearly illustrate the structure, and feel free to use multiple viewpoints if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:34.524957Z",
     "start_time": "2024-09-13T15:16:34.521010Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_synthetic_images(image_dir='./photometrics_images/SphereGray5/', channel=0):\n",
    "    \"\"\"\n",
    "    Loads synthetic images and their corresponding light source directions.\n",
    "\n",
    "    This function reads all images from the specified directory, extracts the relevant channel, and stacks the images along the third dimension. It also extracts the light source directions from the image filenames and normalizes both the image stack and the light source vectors.\n",
    "\n",
    "    Args:\n",
    "        image_dir: directory containing the synthetic images\n",
    "        channel: color channel to be extracted (default is 0, corresponding to the red channel)\n",
    "\n",
    "    Returns:\n",
    "        image_stack: a 3d numpy array of stacked images\n",
    "        normalized_light_dirs: a 2d numpy array of normalized light source directions\n",
    "    \"\"\"\n",
    "    # list all files in the image directory\n",
    "    image_files = os.listdir(image_dir)\n",
    "    num_images = len(image_files)\n",
    "\n",
    "    image_stack = None\n",
    "    light_directions = None\n",
    "    \n",
    "    # assumed z component for the light direction\n",
    "    z_component = 0.5  \n",
    "\n",
    "    # loop through all image files\n",
    "    for i in range(num_images):\n",
    "        # read the input image\n",
    "        image = cv2.imread(os.path.join(image_dir, image_files[i]))\n",
    "        image = np.flip(image, axis=-1)  # flip the image to correct the channel order\n",
    "        image = image[:, :, channel]  # extract the specified color channel\n",
    "\n",
    "        # initialize image stack and light direction array on the first iteration\n",
    "        if image_stack is None:\n",
    "            height, width = image.shape\n",
    "            print('image size (h*w): %d*%d' % (height, width))\n",
    "            image_stack = np.zeros([height, width, num_images], dtype=int)\n",
    "            light_directions = np.zeros([num_images, 3], dtype=np.float64)\n",
    "\n",
    "        # stack the image along the third dimension\n",
    "        image_stack[:, :, i] = image\n",
    "\n",
    "        # extract light direction from the image filename\n",
    "        x_component = np.double(image_files[i][(image_files[i].find('_') + 1):image_files[i].rfind('_')])\n",
    "        y_component = np.double(image_files[i][image_files[i].rfind('_') + 1:image_files[i].rfind('.png')])\n",
    "        light_directions[i, :] = [x_component, -y_component, z_component]\n",
    "\n",
    "    # convert the image stack to double precision for further processing\n",
    "    image_stack = np.double(image_stack)\n",
    "\n",
    "    # normalize the image stack\n",
    "    min_val = np.min(image_stack)\n",
    "    max_val = np.max(image_stack)\n",
    "\n",
    "    # handle the case where all pixel values are the same\n",
    "    if max_val != min_val:\n",
    "        image_stack = (image_stack - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        image_stack = np.zeros(image_stack.shape)  \n",
    "\n",
    "    # normalize the light direction vectors\n",
    "    norm_factors = np.tile(np.sqrt(np.sum(light_directions ** 2, axis=1, keepdims=True)), (1, light_directions.shape[1]))\n",
    "    normalized_light_dirs = light_directions / norm_factors\n",
    "\n",
    "    return image_stack, normalized_light_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:34.562341Z",
     "start_time": "2024-09-13T15:16:34.557359Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_results(albedo, normals, height_map=None, SE=None):\n",
    "    \"\"\"\n",
    "    Displays the albedo, normals, and optionally the height map and surface error (SE).\n",
    "\n",
    "    Args:\n",
    "        albedo: albedo (reflectance) map\n",
    "        normals: surface normal map\n",
    "        height_map: reconstructed height map (optional)\n",
    "        SE: surface error map (optional)\n",
    "    \"\"\"\n",
    "    # stride in the plot, you may want to adjust it to different images\n",
    "    stride = 1\n",
    "\n",
    "    if albedo is not None:\n",
    "        # showing albedo map\n",
    "        fig = plt.figure(figsize=(5, 5))\n",
    "        albedo_max = 1  # normalize albedo values\n",
    "        albedo = albedo / albedo_max\n",
    "        print(\"Albedo shape:\", albedo.shape)\n",
    "        plt.imshow(albedo, cmap=\"gray\")\n",
    "        plt.title('Albedo Map', fontsize=16)\n",
    "        plt.show()\n",
    "\n",
    "    # showing normals as three separate channels\n",
    "    figure = plt.figure(figsize=(10, 10))\n",
    "    ax1 = figure.add_subplot(221)\n",
    "    ax1.imshow(normals[..., 0], vmin=-1, vmax=1, cmap='viridis')\n",
    "    ax1.set_axis_off()\n",
    "    ax1.set_title('Normal Channel x', fontsize=16)\n",
    "    ax2 = figure.add_subplot(222)\n",
    "    ax2.imshow(normals[..., 1], vmin=-1, vmax=1, cmap='viridis')\n",
    "    ax2.set_axis_off()\n",
    "    ax2.set_title('Normal Channel y', fontsize=16)\n",
    "    ax3 = figure.add_subplot(223)\n",
    "    ax3.imshow(normals[..., 2], vmin=-1, vmax=1, cmap='viridis')\n",
    "    ax3.set_axis_off()\n",
    "    ax3.set_title('Normal Channel z', fontsize=16)\n",
    "    ax4 = figure.add_subplot(224)\n",
    "    ax4.imshow((normals + 1) * 0.5)\n",
    "    ax4.set_axis_off()\n",
    "    ax4.set_title('Combined Normals', fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "    # Create meshgrid for plotting 3D surfaces\n",
    "    X, Y, _ = np.meshgrid(np.arange(0, np.shape(normals)[0], stride),\n",
    "                          np.arange(0, np.shape(normals)[1], stride),\n",
    "                          np.arange(1))\n",
    "    X = X[..., 0]\n",
    "    Y = Y[..., 0]\n",
    "\n",
    "    # Show both heightmap and SE in a 1x2 subplot if they're available\n",
    "    if height_map is not None and SE is not None:\n",
    "        fig = plt.figure(figsize=(8, 4))\n",
    "\n",
    "        # Show heightmap\n",
    "        ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "        H_height_map = height_map[::stride, ::stride]\n",
    "        ax1.plot_surface(X, Y, H_height_map.T)\n",
    "        ax1.set_title(\"Height Map\")\n",
    "\n",
    "        # Show SE\n",
    "        ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "        H_SE = SE[::stride, ::stride]\n",
    "        ax2.plot_surface(X, Y, H_SE.T, color='r')\n",
    "        ax2.set_title(\"SE\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Only show heightmap if it's available\n",
    "    elif height_map is not None:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "        H_height_map = height_map[::stride, ::stride]\n",
    "        ax.plot_surface(X, Y, H_height_map.T)\n",
    "        ax.set_title(\"Height Map\")\n",
    "        plt.show()\n",
    "\n",
    "    # Only show SE if it's available\n",
    "    elif SE is not None:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "        H_SE = SE[::stride, ::stride]\n",
    "        ax.plot_surface(X, Y, H_SE.T, color='r')\n",
    "        ax.set_title(\"SE\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the guidelines in *Computer Vision: A Modern Approach* by Forsyth and Ponce, Chapter 5, Section 5.4, you will complete the `estimate_albedo_and_normals()` function by implementing the following steps:\n",
    "\n",
    "1. **Iterate through each point in the image array**:\n",
    "   - For each pixel, extract the intensity values across all images and stack them into a vector `i`.\n",
    "\n",
    "2. **Construct the diagonal matrix**:\n",
    "   - Construct the diagonal matrix `scriptI` using the vector `i`.\n",
    "\n",
    "3. **Solve the linear system**:\n",
    "   - Solve the equation `scriptI * scriptV * g = scriptI * i` to obtain the vector `g` for each pixel.\n",
    "\n",
    "4. **Compute the albedo**:\n",
    "   - The albedo at each pixel is the magnitude (norm) of the vector `g`, denoted as `|g|`.\n",
    "\n",
    "5. **Compute the surface normal**:\n",
    "   - The surface normal at each pixel is given by `g / |g|`, where `|g|` is non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:35.565979Z",
     "start_time": "2024-09-13T15:16:34.591856Z"
    }
   },
   "outputs": [],
   "source": [
    "# BEN'S CODE\n",
    "import torch\n",
    "\n",
    "def estimate_albedo_and_normals(image_stack, scriptV, shadow_trick=True):\n",
    "    \"\"\"\n",
    "    Estimates the surface albedo and normals from a stack of images.\n",
    "\n",
    "    Args:\n",
    "        image_stack: The stack of images of the surface, stacked along the third dimension.\n",
    "        scriptV: The matrix containing the source and camera information.\n",
    "        shadow_trick: Boolean indicating whether to use the shadow trick (default is True).\n",
    "\n",
    "    Returns:\n",
    "        albedo: The estimated surface albedo.\n",
    "        normal: The estimated surface normals.\n",
    "    \"\"\"\n",
    "\n",
    "    h, w, n = image_stack.shape\n",
    "\n",
    "    # create arrays for the albedo and normals\n",
    "    albedo = np.zeros([h, w])\n",
    "    normal = np.zeros([h, w, 3])\n",
    "\n",
    "    albedo_vectorised = np.zeros([h, w])\n",
    "    normal_vectorised = np.zeros([h, w, 3])\n",
    "    \n",
    "    if shadow_trick:\n",
    "        diagonal_matrices = np.zeros((h, w, n, n))\n",
    "        diagonal_matrices[:, :, np.arange(n), np.arange(n)] = image_stack\n",
    "        a = diagonal_matrices @ scriptV\n",
    "        b = diagonal_matrices @ image_stack[..., None]\n",
    "    else:\n",
    "        a = np.tile(scriptV, (h, w, 1, 1))\n",
    "        b = image_stack[..., None]\n",
    "        \n",
    "\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    a_torch = torch.tensor(a)\n",
    "    b_torch = torch.tensor(b)\n",
    "\n",
    "    # Solve the least squares for each element in the batch\n",
    "    x_torch = torch.linalg.lstsq(a_torch, b_torch).solution.squeeze(-1)\n",
    "    albedo_vectorised = torch.norm(x_torch, dim=-1).numpy()\n",
    "\n",
    "    # Create a mask for the non-zero norms\n",
    "    mask_non_zero = albedo_vectorised != 0\n",
    "\n",
    "    # Perform division where norm is nonzero\n",
    "    normal_vectorised[mask_non_zero] = x_torch[mask_non_zero] / albedo_vectorised[..., None][mask_non_zero]\n",
    "\n",
    "    # This is a non-vecorised version if we are not allowed to use torch\n",
    "\n",
    "    '''\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            # Stack the pixel values into a single vector\n",
    "            boldi = image_stack[i, j]\n",
    "\n",
    "            # Apply the shadow trick\n",
    "            if shadow_trick:\n",
    "                scriptI = np.diag(boldi)\n",
    "                boldg = np.linalg.lstsq(scriptI @ scriptV, scriptI @ boldi, rcond=None)[0]\n",
    "            else:\n",
    "                boldg = np.linalg.lstsq(scriptV, boldi, rcond=None)[0]\n",
    "\n",
    "            # Compute the albedo according to the formula\n",
    "            albedo[i, j] = np.linalg.norm(boldg)\n",
    "\n",
    "            # Check if the albedo is zero to avoid division by zero     \n",
    "            if albedo[i, j] > 0:\n",
    "                # Compute the surface normal\n",
    "                normal[i, j] = boldg / albedo[i, j]\n",
    "    '''\n",
    "\n",
    "    albedo = albedo_vectorised\n",
    "    normal = normal_vectorised\n",
    "    \n",
    "    return albedo, normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:35.572450Z",
     "start_time": "2024-09-13T15:16:35.569708Z"
    }
   },
   "outputs": [],
   "source": [
    "# PRAD'S CODE\n",
    "def estimate_albedo_and_normals(image_stack, scriptV, shadow_trick=True):\n",
    "    \"\"\"\n",
    "    Estimates the surface albedo and normals from a stack of images.\n",
    "\n",
    "    Args:\n",
    "        image_stack: The stack of images of the surface, stacked along the third dimension.\n",
    "        scriptV: The matrix containing the source and camera information.\n",
    "        shadow_trick: Boolean indicating whether to use the shadow trick (default is True).\n",
    "\n",
    "    Returns:\n",
    "        albedo: The estimated surface albedo.\n",
    "        normal: The estimated surface normals.\n",
    "    \"\"\"\n",
    "\n",
    "    h, w, n = image_stack.shape\n",
    "\n",
    "    # create arrays for the albedo and normals\n",
    "    albedo = np.zeros([h, w])\n",
    "    normal = np.zeros([h, w, 3])\n",
    "\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            # Stack the pixel values into a single vector\n",
    "            boldi = image_stack[i, j]\n",
    "\n",
    "            # Apply the shadow trick\n",
    "            if shadow_trick:\n",
    "                scriptI = np.diag(boldi)\n",
    "                boldg = np.linalg.lstsq(scriptI @ scriptV, scriptI @ boldi, rcond=None)[0]\n",
    "            else:\n",
    "                boldg = np.linalg.lstsq(scriptV, boldi, rcond=None)[0]\n",
    "\n",
    "            # Compute the albedo according to the formula\n",
    "            albedo[i, j] = np.linalg.norm(boldg)\n",
    "\n",
    "            # Check if the albedo is zero to avoid division by zero     \n",
    "            if albedo[i, j] > 0:\n",
    "                # Compute the surface normal\n",
    "                normal[i, j] = boldg / albedo[i, j]\n",
    "    \n",
    "    return albedo, normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:38.635527Z",
     "start_time": "2024-09-13T15:16:35.579016Z"
    }
   },
   "outputs": [],
   "source": [
    "# load synthetic images for SphereGray5\n",
    "image_stack, normalized_light_dirs = load_synthetic_images()\n",
    "print(image_stack.shape, normalized_light_dirs.shape)\n",
    "#indices = [0, 1, 3]\n",
    "#image_stack = image_stack[..., indices]\n",
    "print(image_stack.shape)\n",
    "#normalized_light_dirs = normalized_light_dirs[indices]\n",
    "\n",
    "# estimate albedo and normals\n",
    "albedo, normals = estimate_albedo_and_normals(image_stack, normalized_light_dirs)\n",
    "\n",
    "# show results\n",
    "show_results(albedo, normals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:39.023366Z",
     "start_time": "2024-09-13T15:16:38.659932Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(image_stack[:, :, 0], cmap='gray')\n",
    "plt.title('First Image in Image Stack')\n",
    "plt.show()\n",
    "plt.imshow(image_stack[:, :, 1], cmap='gray')\n",
    "plt.title('Second Image in Image Stack')\n",
    "plt.show()\n",
    "plt.imshow(image_stack[:, :, 2], cmap='gray')\n",
    "plt.title('Third Image in Image Stack')\n",
    "plt.show()\n",
    "plt.imshow(image_stack[:, :, 3], cmap='gray')\n",
    "plt.title('Fourth Image in Image Stack')\n",
    "plt.show()\n",
    "plt.imshow(image_stack[:, :, 4], cmap='gray')\n",
    "plt.title('Fifth Image in Image Stack')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:39.342076Z",
     "start_time": "2024-09-13T15:16:39.074842Z"
    }
   },
   "outputs": [],
   "source": [
    "img_stack_first = image_stack[:, :, :5]\n",
    "height, width, n = img_stack_first.shape\n",
    "print(f\"Height: {height}, Width: {width}, Number of images: {n}\")\n",
    "# Loop through all images\n",
    "#for img in range(n):\n",
    "count = 0\n",
    "for img in range(n):\n",
    "  #print(f\"Image {img}:\")\n",
    "  for i in range(height):\n",
    "    for j in range(width):\n",
    "      #print(f\"Pixel value at ({i}, {j}): {img_stack_first[i, j, img]}\")\n",
    "      # Stack the pixel values into a single array\n",
    "      pixel_values = img_stack_first[i, j, :]\n",
    "      #print(f\"Pixel values at ({i}, {j}): {pixel_values}\")\n",
    "      count += 1\n",
    "  print(f\"Pixel values for image {img}: {pixel_values}\")\n",
    "#print(f\"Total number of pixels: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:39.358767Z",
     "start_time": "2024-09-13T15:16:39.356558Z"
    }
   },
   "outputs": [],
   "source": [
    "pixel_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-16\"></a>\n",
    "#### <font color='#FF0000'>Question 16 (2 points)</font>\n",
    "\n",
    "After implementing the `estimate_albedo_and_normals()` function, what do you expect to see in the albedo image? How does this compare to your actual result? Explain the differences.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*A uniformly lit circle. The results matches this expectation closely.\n",
    ">\n",
    ">We would expect there to be a checkered circle, i.e. sphere with 4 quadrants in two different sharings, that for each segment is either uniformly dark grey or uniformly light grey. Then we would expect to see the background behind the sphere to be uniformly black. What we see is pretty in line with this expectation. The background behind the circle is indeed uniformly black and the quadrants of the circumference are mostly as expected.\n",
    ">\n",
    "> The one small difference from what we expected is that towards the circumference of the circle there is very slight shading in the quadrants that gives the appearance of it being a 3D shape. On closer inspection of the 5 images this makes sense as the same shading appears there in some of the images but there are also a few where the appearance of depth is absent. It seems that the albedo calculation is dominated by the first image in particular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-17\"></a>\n",
    "#### <font color='#FF0000'>Question 17 (1 point)</font>\n",
    "\n",
    "In principle, what is the minimum number of images you need to estimate the albedo and surface normal? Explain your reasoning.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*The g vector is of shape (3,1). This means that there are 3 unknows and thus 3 equations are needed to solve this systen of equations. Hence, at least 3 pictures are needed. But note that 3 pictures may not be sufficient. This is bourne out empirically, for example in the case of SphereGray5 where of the ${5 \\choose 3} = 10$ possible indices combinations, only two result in an albedo that visually looks the same as the one for 5 images, namely with indices $[0, 1, 2]$ and $[2, 3, 4]$. More interestingly, of the ${5 \\choose 4} = 5$ possible indices (again in the case of SphereGray5), no combination of the 5 images results in an albedo that looks the same. Hence, it seems to heavily depend on the specific images used. Indeed we can still assert that at least 3 images are needed as any less than this and no combination of images can give an albedo that looks the same as for the 5 images. However, it does depend on which images.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-18\"></a>\n",
    "#### <font color='#FF0000'>Question 18 (2 points)</font>\n",
    "\n",
    "Now, run the algorithm with more images by using the SphereGray25 folder. Observe the differences in the results when using more images. Report your findings. \n",
    "\n",
    "You could try all images at once or a few at a time, incrementally. Choose a strategy and justify it by discussing your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:42.997663Z",
     "start_time": "2024-09-13T15:16:39.400429Z"
    }
   },
   "outputs": [],
   "source": [
    "# load synthetic images for SphereGray25\n",
    "image_stack_25, normalized_light_dirs_25 = load_synthetic_images('./photometrics_images/SphereGray25/')\n",
    "#indices = [20, 21, 22, 23, 24]\n",
    "#image_stack_25 = image_stack_25[..., indices]\n",
    "#normalized_light_dirs_25 = normalized_light_dirs_25[indices]\n",
    "\n",
    "# estimate albedo and normals for SphereGray25\n",
    "albedo_25, normals_25 = estimate_albedo_and_normals(image_stack_25, normalized_light_dirs_25)\n",
    "\n",
    "# show the results for SphereGray25\n",
    "show_results(albedo, normals)\n",
    "show_results(albedo_25, normals_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:43.129028Z",
     "start_time": "2024-09-13T15:16:43.022168Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(albedo_25 - albedo, cmap='jet')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*The plot above shows the difference between albedo25 and albedo5. It can be observed that the white sectios of the sphere have larger values. This means that albedo for the 25-image version has higher values. The whites are more reflective than in the 5-image counterpart. If we run the algorithm with subsets of size 5 of the set of 25 images we can substantially different albedos and normals in comparison to SphereGray5 supporting once again the conclusion that the specific images used matter greatly in determining the quality of the albedos and surface normals.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-19\"></a>\n",
    "#### <font color='#FF0000'>Question 19 (2 points)</font>\n",
    "\n",
    "Consider the impact of shadows in photometric stereo. Explain the trick mentioned in the textbook to deal with shadows.\n",
    "\n",
    "Remove this trick from your implementation and check your results. Is the trick necessary when using 5 images? How about when using 25 images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:53.636112Z",
     "start_time": "2024-09-13T15:16:43.154579Z"
    }
   },
   "outputs": [],
   "source": [
    "# load synthetic images for SphereGray5 and SphereGray25\n",
    "image_stack, normalized_light_dirs = load_synthetic_images()\n",
    "image_stack_25, normalized_light_dirs_25 = load_synthetic_images('./photometrics_images/SphereGray25/')\n",
    "\n",
    "# estimate albedo and normals without shadow trick\n",
    "albedo_s5, normals_s5 = estimate_albedo_and_normals(image_stack, normalized_light_dirs, False)\n",
    "albedo_s5_st, normals_s5_st = estimate_albedo_and_normals(image_stack, normalized_light_dirs, True)\n",
    "albedo_s25, normals_s25 = estimate_albedo_and_normals(image_stack_25, normalized_light_dirs_25, False)\n",
    "albedo_s25_st, normals_s25_st = estimate_albedo_and_normals(image_stack_25, normalized_light_dirs_25, False)\n",
    "# show results without shadow trick\n",
    "show_results(albedo_s5, normals_s5)\n",
    "show_results(albedo_s5_st, normals_s5_st)\n",
    "show_results(albedo_s25, normals_s25)\n",
    "show_results(albedo_s25_st, normals_s25_st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots:\n",
    "1. 5 images, ST=off\n",
    "2. 5 images, ST=on \n",
    "3. 25 images, ST=off\n",
    "4. 25 images, ST=on\n",
    ">*For the 25 image version, the shadow trick makes no difference. For the 5 image version the shadow trick helps with the smoothness of the transitions of the shadows. With the shadow trick turned off there are abrupt ends to the shadows, whereas these are smooth transitions with the shadow trick enabled.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-20\"></a>\n",
    "#### <font color='#FF0000'>Question 20 (5 points)</font>\n",
    "\n",
    "Before reconstructing the surface height map, it is necessary to compute the partial derivatives, $\\frac{\\partial f}{\\partial x}$ and $\\frac{\\partial f}{\\partial y}$ (referred to as *p* and *q* in the algorithm). These partial derivatives also allow for a double-check of the computation through a test of *integrability*.\n",
    "\n",
    "In this question, you will need to complete the function `check_integrability()` to compute the partial derivatives *p* and *q*.\n",
    "\n",
    "Make sure to verify your results by examining the integrability of the computed derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:53.670528Z",
     "start_time": "2024-09-13T15:16:53.662573Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_integrability(normals):    \n",
    "    \"\"\"\n",
    "    Checks the surface gradient for integrability.\n",
    "\n",
    "    This function computes the partial derivatives of the surface function with respect to x and y \n",
    "    (p and q), and calculates the squared errors of the mixed partial derivatives to test for integrability.\n",
    "\n",
    "    Args:\n",
    "        normals: The normal map of the surface.\n",
    "\n",
    "    Returns:\n",
    "        p: The partial derivative of the surface function with respect to x (df/dx).\n",
    "        q: The partial derivative of the surface function with respect to y (df/dy).\n",
    "        SE: The squared errors of the mixed partial derivatives.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize p, q, and SE\n",
    "    p = np.zeros(normals.shape[:2])\n",
    "    q = np.zeros(normals.shape[:2])\n",
    "    SE = np.zeros(normals.shape[:2])\n",
    "\n",
    "    # Create a mask for the non-zero norms\n",
    "    mask_non_zero = normals[ : , : , 2] != 0\n",
    "\n",
    "    p[mask_non_zero] = normals[ : , : , 0][mask_non_zero] / normals[ : , : , 2][mask_non_zero]\n",
    "    q[mask_non_zero] = normals[ : , : , 1][mask_non_zero] / normals[ : , : , 2][mask_non_zero]\n",
    "\n",
    "    SE = (np.gradient(p, axis=1) - np.gradient(q, axis=0)) ** 2\n",
    "\n",
    "    return -p, -q, SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:53.715142Z",
     "start_time": "2024-09-13T15:16:53.702990Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the normals that you found in the previous question\n",
    "p, q, SE = check_integrability(normals_s5)\n",
    "\n",
    "print('Showing the integrability check results for 5 light sources:')\n",
    "print(f'SE max: {SE.max()}\\n')\n",
    "\n",
    "p, q, SE = check_integrability(normals_s25)\n",
    "\n",
    "print('Showing the integrability check results for 25 light sources:')\n",
    "print(f'SE max: {SE.max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-21\"></a>\n",
    "#### <font color='#FF0000'>Question 21 (3 points)</font>\n",
    "\n",
    "Implement and compute the second derivatives according to the provided algorithm, and perform the test of integrability by applying a reasonable threshold to the squared errors. \n",
    "\n",
    "Reflect on the potential causes of any errors observed. Additionally, analyze how the test of integrability performs when using the GraySphere5 and GraySphere25 datasets. Use the normal maps obtained from the previous steps with the shadow trick applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:54.682200Z",
     "start_time": "2024-09-13T15:16:53.823116Z"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# here we include the arrays with and without the shadow trick\n",
    "albdlst = [albedo_s5_st, albedo_s25_st]\n",
    "normlst = [normals_s5_st, normals_s25_st]\n",
    "threshhold = 0.01\n",
    "\n",
    "for a, n in zip(albdlst, normlst):\n",
    "    _,_,SE_    = check_integrability(n)\n",
    "    s = SE_.shape\n",
    "    SE_[SE_ < threshhold] = np.nan\n",
    "    SE_.reshape(s)\n",
    "    print(SE_)\n",
    "    show_results(a, n, SE=SE_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflect on the potential causes of any errors observed. Additionally, analyze how the test of integrability performs when using the GraySphere5 and GraySphere25 datasets. Use the normal maps obtained from the previous steps with the shadow trick applied.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>\n",
    "\n",
    "The squared errors are localized around the circumference of the sphere. As we add more images, the magnitude of the error decreases.\n",
    "\n",
    "Since the shape is a sphere, the edges are less well-defined (always containing a slight shadow or higher albedo where the light hits). Since there are no examples that show the edges in perfect relief (i.e. no shadows or highlights), the error is higher at the edges. In contrast, we expect a shape like a cube might have lower error at the edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-22\"></a>\n",
    "#### <font color='#FF0000'>Question 22 (6 points)</font>\n",
    "\n",
    "To reconstruct the surface height map, you need to integrate the partial derivatives over a path. Since we are working with discrete structures, this integration is done by summing their values.\n",
    "\n",
    "The algorithm presented in the chapter suggests performing the integration in a column-major order: start at the top-left corner, integrate along the first column, and then proceed to the right along each row. However, it is also recommended to use multiple paths and average the results to help distribute the errors from derivative estimates.\n",
    "\n",
    "In this question, you will:\n",
    "\n",
    "1. Construct the surface height map using column-major order as described in the algorithm.\n",
    "2. Implement the row-major path integration method.\n",
    "\n",
    "Your implementation should be added to the `construct_surface()` function.\n",
    "\n",
    "**Note**: By default, Numpy uses row-major operations. If you unroll an image to linearize the operation, you will end up with a row-major representation. Numpy can be configured to use column-major order, but this concern does not apply if you are using double for-loops without unrolling.\n",
    "\n",
    "**Hint**: To further inspect the shape of the objects and normal directions, consider using the `matplotlib.pyplot.quiver` function. Be sure to choose appropriate sub-sampling ratios for proper illustration. Add this to the `show_results()` function if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:54.741803Z",
     "start_time": "2024-09-13T15:16:54.736973Z"
    }
   },
   "outputs": [],
   "source": [
    "def construct_surface(p, q, path_type='column'):\n",
    "    \"\"\"\n",
    "    Constructs the surface function represented as height_map.\n",
    "\n",
    "    This function integrates the partial derivatives p and q to reconstruct the surface height map.\n",
    "    The integration follows the pseudocode provided.\n",
    "\n",
    "    Args:\n",
    "        p: The partial derivative of the surface function with respect to x (df/dx).\n",
    "        q: The partial derivative of the surface function with respect to y (df/dy).\n",
    "        path_type: The type of path to construct the height_map, either 'column', 'row', or 'average'.\n",
    "\n",
    "    Returns:\n",
    "        height_map: The reconstructed surface height map.\n",
    "    \"\"\"\n",
    "\n",
    "    h, w = p.shape\n",
    "    height_map = np.zeros([h, w])  # Top left corner of height map is zero\n",
    "\n",
    "    if path_type == 'column':\n",
    "        # For each pixel in the left column of height map\n",
    "        for i in range(1, h):\n",
    "            height_map[i, 0] = height_map[i-1, 0] + q[i, 0]\n",
    "        \n",
    "        # For each row\n",
    "        for i in range(h):\n",
    "            # For each element of the row except for leftmost\n",
    "            for j in range(1, w):\n",
    "                height_map[i, j] = height_map[i, j-1] + p[i, j]\n",
    "\n",
    "    elif path_type == 'row':\n",
    "        # For each pixel in the top row of height map\n",
    "        for j in range(1, w):\n",
    "            height_map[0, j] = height_map[0, j-1] + p[0, j]\n",
    "        \n",
    "        # For each column\n",
    "        for j in range(w):\n",
    "            # For each element of the column except for topmost\n",
    "            for i in range(1, h):\n",
    "                height_map[i, j] = height_map[i-1, j] + q[i, j]\n",
    "\n",
    "    elif path_type == 'average':\n",
    "        # Calculate column-major path\n",
    "        height_map_column = construct_surface(p, q, \"column\")\n",
    "\n",
    "        # Calculate row-major path\n",
    "        height_map_row = construct_surface(p, q, \"row\")\n",
    "\n",
    "        # Average the two paths\n",
    "        height_map = (height_map_column + height_map_row) / 2\n",
    "    \n",
    "    #print(f\"Height map: \\n {height_map}\")\n",
    "    #print(f\"Shape of height map: {height_map.shape}\")\n",
    "\n",
    "    return height_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:54.783555Z",
     "start_time": "2024-09-13T15:16:54.775154Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make a random 3x3 matrix in the range 1-10\n",
    "random_matrix = np.random.rand(3, 3) * 10 + 1\n",
    "random_matrix = np.round(random_matrix)\n",
    "q = np.gradient(random_matrix, axis=1)\n",
    "# Print the matrix\n",
    "print(f\"Random Matrix: \\n {random_matrix}\")\n",
    "print(f\"q: \\n {q}\")\n",
    "\n",
    "# for i in range(3):\n",
    "#     for j in range(3):\n",
    "#         print(f\"Element at position ({i}, {j}): {random_matrix[i, j]}\")\n",
    "\n",
    "height_map = np.zeros([3, 3])\n",
    "for i in range(1,3):\n",
    "    height_map[i, 0] = height_map[i-1, 0] + q[i, 0]\n",
    "    print(f\"Height map at position ({i}, 0): {height_map[i, 0]}\")\n",
    "\n",
    "print(f\"Height map: \\n {height_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:55.659993Z",
     "start_time": "2024-09-13T15:16:54.810644Z"
    }
   },
   "outputs": [],
   "source": [
    "p, q, SE = check_integrability(normals_s25)\n",
    "\n",
    "height_map = construct_surface(p, q, \"column\")\n",
    "\n",
    "# set the threshold for visualization\n",
    "threshold = 0.01\n",
    "SE[SE <= threshold] = float('nan')\n",
    "\n",
    "print('Showing the surface reconstruction results for 5 light sources using column-major integration:')\n",
    "show_results(None, normals_s5, height_map, SE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-23\"></a>\n",
    "#### <font color='#FF0000'>Question 23 (2 points)</font>\n",
    "\n",
    "Compare the results obtained from the two different integration paths (column-major and row-major). \n",
    "\n",
    "What are the differences in the reconstructed surface height maps when using these two paths? Discuss any observations you notice between the two approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:57.009313Z",
     "start_time": "2024-09-13T15:16:55.784218Z"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "p, q, SE = check_integrability(normals_s25)\n",
    "\n",
    "height_map = construct_surface(p, q, \"column\")\n",
    "\n",
    "# set the threshold for visualization\n",
    "threshold = 0.01\n",
    "SE[SE <= threshold] = float('nan')\n",
    "\n",
    "print('Showing the surface reconstruction results for 25 light sources using column-major integration:')\n",
    "show_results(None, normals_s25, height_map, SE)\n",
    "\n",
    "\n",
    "p, q, SE = check_integrability(normals_s25)\n",
    "\n",
    "height_map = construct_surface(p, q, \"row\")\n",
    "\n",
    "# set the threshold for visualization\n",
    "threshold = 0.01\n",
    "SE[SE <= threshold] = float('nan')\n",
    "\n",
    "print('Showing the surface reconstruction results for 25 light sources using row-major integration:')\n",
    "show_results(None, normals_s25, height_map, SE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*The row and column major order both show distinct stripes along the surface of the sphere. The stripes run in orthogonal direction in row major order compared to column major order.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-24\"></a>\n",
    "#### <font color='#FF0000'>Question 24 (3 points)</font>\n",
    "\n",
    "Now, take the average of the results obtained from the column-major and row-major integration paths.\n",
    "\n",
    "Do you observe any improvement compared to using only one path? Additionally, analyze whether the construction results vary when using different numbers of images in the reconstruction process. (So compare the results obtained from the GraySphere5 and GraySphere25 datasets.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:58.615396Z",
     "start_time": "2024-09-13T15:16:57.125322Z"
    }
   },
   "outputs": [],
   "source": [
    "p, q, SE = check_integrability(normals_s5)\n",
    "\n",
    "height_map = construct_surface(p, q, \"average\")\n",
    "\n",
    "# set the threshold for visualization\n",
    "threshold = 0.01\n",
    "SE[SE <= threshold] = float('nan')\n",
    "\n",
    "print('Showing the surface reconstruction results for 5 light sources using average integration:')\n",
    "show_results(None, normals_s25, height_map, SE)\n",
    "\n",
    "\n",
    "p, q, SE = check_integrability(normals_s25)\n",
    "\n",
    "height_map = construct_surface(p, q, \"average\")\n",
    "\n",
    "# set the threshold for visualization\n",
    "threshold = 0.01\n",
    "SE[SE <= threshold] = float('nan')\n",
    "\n",
    "print('Showing the surface reconstruction results for 25 light sources using average integration:')\n",
    "show_results(None, normals_s5, height_map, SE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*The distinct stripes from Q23 are largely gone when averaging over the two orders. There is no major difference between the 5 and 25 image version in the heightmap*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-25\"></a>\n",
    "#### <font color='#FF0000'>Question 25 (3 points)</font>\n",
    "\n",
    "Run the photometric stereo algorithm on the MonkeyGray model using the `photometric_stereo` function and display the results. Complete the provided code, execute the algorithm, and show the outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:16:58.926778Z",
     "start_time": "2024-09-13T15:16:58.923854Z"
    }
   },
   "outputs": [],
   "source": [
    "def photometric_stereo(image_dir='./photometrics_images/MonkeyGray/'):\n",
    "    \"\"\"\n",
    "    Performs photometric stereo to estimate the surface albedo, normals, and height map for a given set of images under varying illumination conditions.\n",
    "\n",
    "    Args:\n",
    "        image_dir: Directory containing the input images (default is './images/photometrics_images/MonkeyGray/').\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # obtain many images in a fixed view under different illumination\n",
    "    print('Loading images...\\n')\n",
    "    [image_stack, scriptV] = load_synthetic_images(image_dir)\n",
    "    [h, w, n] = image_stack.shape\n",
    "    print('Finish loading %d images.\\n' % n)\n",
    "\n",
    "    # compute the surface gradient from the stack of images and light source matrix\n",
    "    print('Computing surface albedo and normal map...\\n')\n",
    "\n",
    "    albedo, normals = estimate_albedo_and_normals(image_stack, scriptV)\n",
    "\n",
    "    # integrability check: is (dp / dy  -  dq / dx) ^ 2 small everywhere?\n",
    "    print('Integrability checking\\n')\n",
    "\n",
    "    p, q, SE = check_integrability(normals)\n",
    "\n",
    "    # set the threshold for visualization\n",
    "    threshold = 0.01\n",
    "    SE[SE <= threshold] = float('nan')\n",
    "\n",
    "\n",
    "    print('Number of outliers: %d\\n' % np.sum(SE > threshold))\n",
    "    SE[SE <= threshold] = float('nan')  # for good visualization\n",
    "\n",
    "    # compute the surface height\n",
    "    print('Computing surface height map...\\n')\n",
    "\n",
    "    height_map = construct_surface(p, q, \"column\")\n",
    "\n",
    "    # show results\n",
    "    print('Displaying results...\\n')\n",
    "\n",
    "    show_results(albedo, normals, height_map, SE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you estimate the albedo and surface normal for MonkeyGrat, the cell below will run for at least 20 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:17:33.329358Z",
     "start_time": "2024-09-13T15:16:58.948770Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the function\n",
    "image_dir = './photometrics_images/MonkeyGray/'\n",
    "photometric_stereo(image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-26\"></a>\n",
    "#### <font color='#FF0000'>Question 26 (2 points)</font>\n",
    "\n",
    "The albedo results for the MonkeyGray model may exhibit more errors compared to the sphere. Observe and describe the errors that arise when running the photometric stereo algorithm on the MonkeyGray model.\n",
    "\n",
    "What could be the reasons for these errors? Experiment with different numbers of images, as you did in Question 15 and Question 18, to see the effects and discuss your observations.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Because the monkey's geometry is far more complex than that of the sphere, there is a higher chance that the surface casts shadows onto itself, which causes the albedo map to not be perfectly uniform. This is especially noticable around the eyes. This affects the accuracy of the albedo map which also affects the calcultion of the normal vectors. These inaccuracies could be responsible for the increased errors.*\n",
    "\n",
    "#TODO: comment on effects with dif numbers of images - more SE but better height map with more images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-27\"></a>\n",
    "#### <font color='#FF0000'>Question 27 (2 points)</font>\n",
    "\n",
    "Considering the errors observed in the albedo results for the MonkeyGray model, what do you think could help in reducing or solving these errors? Provide your thoughts on potential strategies or methods that might improve the accuracy of the results.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*One of the main problems with the albedo map of the monkey is that it is not completely uniform. One of the possible reasons for this are the shadows casted on the surface by the geometry of the monkey, as mentioned in Q26. One possible solution for this would be to introduce additional light sources where the eyes, and other probematic areas are evenly lit.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-28\"></a>\n",
    "#### <font color='#FF0000'>Question 28 (5 points)</font>\n",
    "\n",
    "So far, we have assumed that albedos and input images are 1-channel grayscale images. To work with 3-channel RGB images, a simple approach is to split the input image into separate channels and treat them individually. However, this method may introduce issues when constructing the surface normal map if a pixel value in a channel is zero.\n",
    "\n",
    "Update the implementation to work with 3-channel RGB inputs. Complete the functions `load_synthetic_images_rgb` and `estimate_albedo_and_normals_rgb ` to handle RGB images, and test your updated implementation with the SphereColor and MonkeyColor models using the `photometric_stereo_rgb` function. Make sure to display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:17:33.368233Z",
     "start_time": "2024-09-13T15:17:33.363497Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_synthetic_images_rgb(image_dir):\n",
    "    \"\"\"\n",
    "    Loads synthetic RGB images and light directions.\n",
    "\n",
    "    This function reads images from the specified directory, processes each color channel separately, \n",
    "    and stacks them together along with their corresponding light directions.\n",
    "\n",
    "    Args:\n",
    "        image_dir: Path to the directory containing the images.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the image stack and the light directions.\n",
    "    \"\"\"\n",
    "\n",
    "    image_stack_rgb, scriptV_rgb = [], []\n",
    "\n",
    "    for channel in range(3):\n",
    "        image_stack, scriptV = load_synthetic_images(image_dir, channel=channel)\n",
    "        image_stack_rgb.append(image_stack)\n",
    "        scriptV_rgb.append(scriptV)\n",
    "\n",
    "    image_stack_rgb = np.stack(image_stack_rgb, axis=-1)\n",
    "    scriptV_rgb = np.stack(scriptV_rgb, axis=-1)\n",
    "\n",
    "    return image_stack_rgb, scriptV_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:17:33.427768Z",
     "start_time": "2024-09-13T15:17:33.423289Z"
    }
   },
   "outputs": [],
   "source": [
    "def estimate_albedo_and_normals_rgb(image_stack, scriptV, shadow_trick=True):\n",
    "    \"\"\"\n",
    "    Estimates albedo and normals from a stack of RGB images and light directions.\n",
    "\n",
    "    This function processes each color channel separately to estimate the albedo and normal maps.\n",
    "    The results from each channel are then combined to produce the final albedo and normal maps.\n",
    "\n",
    "    Args:\n",
    "        image_stack: Image stacks for each color channel.\n",
    "        scriptV: Light directions for each color channel.\n",
    "        shadow_trick: Whether or not to use the shadow trick. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the albedo and normal maps.\n",
    "    \"\"\"\n",
    "    # estimate albedo and normals\n",
    "    albedo_rgb, normals_rgb = [], []\n",
    "    for channel in range(3):\n",
    "        albedo, normals = estimate_albedo_and_normals(image_stack[..., channel], scriptV[..., channel], shadow_trick)\n",
    "        albedo_rgb.append(albedo)\n",
    "        normals_rgb.append(normals)\n",
    "\n",
    "    albedo_rgb = np.stack(albedo_rgb, axis=-1)\n",
    "    normals_rgb = np.stack(normals_rgb, axis=-1)\n",
    "    normals_rgb = np.mean(normals_rgb, axis=-1)\n",
    "\n",
    "    return albedo_rgb, normals_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:17:33.440451Z",
     "start_time": "2024-09-13T15:17:33.435191Z"
    }
   },
   "outputs": [],
   "source": [
    "def photometric_stereo_rgb(image_dir='./photometrics_images/SphereColor/'):\n",
    "    \"\"\"\n",
    "    Performs photometric stereo on RGB images to estimate the surface albedo, normals, and height map.\n",
    "\n",
    "    This function loads RGB images from the specified directory, computes the surface albedo and \n",
    "    normal maps, checks integrability, reconstructs the surface height map, and displays the results.\n",
    "\n",
    "    Args:\n",
    "        image_dir: Path to the directory containing the images.\n",
    "    \"\"\"\n",
    "    image_stack_rgb, scriptV_rgb = load_synthetic_images_rgb(image_dir)\n",
    "\n",
    "    print('Computing surface albedo and normal map...\\n')\n",
    "    albedo_rgb, normals_rgb = estimate_albedo_and_normals_rgb(image_stack_rgb, scriptV_rgb, False)\n",
    "    \n",
    "    print(f\"Shape of albedo_rgb: {albedo_rgb.shape}\")\n",
    "    print(f\"Shape of normals_rgb: {normals_rgb.shape}\")\n",
    "    \n",
    "    \n",
    "    print('Integrability checking\\n')\n",
    "    p_rgb, q_rgb, SE_rgb = check_integrability(normals_rgb)\n",
    "    \n",
    "    # set the threshold for visualization\n",
    "    threshold = 0.01\n",
    "    \n",
    "    SE_rgb[SE_rgb <= threshold] = float('nan')\n",
    "    \n",
    "    print('Number of outliers: %d\\n' % np.sum(SE_rgb > threshold))\n",
    "    \n",
    "    print('Computing surface height map...\\n')\n",
    "    height_map_rgb = construct_surface(p_rgb, q_rgb, path_type=\"column\")\n",
    "    \n",
    "    print('Displaying results...\\n')\n",
    "    show_results(albedo_rgb, normals_rgb, height_map_rgb, SE_rgb)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you estimate the albedo and surface normal for SphereColor and MonkeyColor, the cell below will run for atleast 1 or 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:17:52.177989Z",
     "start_time": "2024-09-13T15:17:33.463145Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the function\n",
    "photometric_stereo_rgb()\n",
    "image_dir = './photometrics_images/SphereColor/'\n",
    "image_dir = './photometrics_images/MonkeyColor/'\n",
    "photometric_stereo_rgb(image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-29\"></a>\n",
    "#### <font color='#FF0000'>Question 29 (2 points)</font>\n",
    "\n",
    "Explain the changes made to the function and discuss the results of these changes.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*The main changes made to the function were to load the images for each channel separately and then stack them together. The albedo and normals were then calculated for each channel separately and then averaged to get the final albedo and normal maps. The results show that the albedo and normal maps are very similar to the ones obtained for the grayscale images. The integrability check shows that the errors are localized around the edges of the sphere and the monkey. Compared to the greyscale images, the squared error has a higher magnitude and is distributed along the edges of both image types. Colour variances and averaging of the albedo could contribute to these higher squared errors. *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-30\"></a>\n",
    "#### <font color='#FF0000'>Question 30 (2 points)</font>\n",
    "\n",
    "Observe the problem in the constructed surface normal map and height map. Explain why a zero pixel could be a problem and propose a way to overcome that.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The constructed surface normal map and height map have a problem where a zero pixel could be a problem. This is because the partial derivatives are calculated by dividing by the normal vector. If the normal vector is zero, this division is undefined. However, in our implementation, the zeroes in the surface normals are masked out, leading to stable calculations. Another way to overcome this problem is to add a small value to the denominator to avoid division by zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-31\"></a>\n",
    "#### <font color='#FF0000'>Question 31 (4 points)</font>\n",
    "\n",
    "Now, it's time to apply the algorithm to real-world datasets. We will first use the Yale Face Database.\n",
    "\n",
    "For the Face dataset, you should:\n",
    "- Load the images\n",
    "- Compute the surface albedo and normal map\n",
    "- Run the integrability check\n",
    "- Find the number of outliers\n",
    "- Compute the surface height\n",
    "- Show the results\n",
    "\n",
    "Run the algorithm for the Yale Face images: [Yale Face Database](http://cvc.cs.yale.edu/cvc/projects/yalefaces/yalefaces.html). The Yale face data is included in the lab material. The `load_face_images` function is provided as a helper. You should complete the `photometric_stereo_face_dataset` function as part of this task.\n",
    "\n",
    "Make sure to display the results for the Face dataset using the 3 paths (column-major, row-major, and average) when constructing the surface height map.\n",
    "\n",
    "**Hint**: For proper computation of albedo and surface normal, you may want to suspend the shadow trick described in the text, and use the original formula:\n",
    "$$i = Vg(x,y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:17:52.263290Z",
     "start_time": "2024-09-13T15:17:52.258504Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_face_images(image_dir='./photometrics_images/yaleB02/'):\n",
    "    \"\"\"\n",
    "    Loads a set of face images from the Yale Face Database and computes corresponding light directions.\n",
    "\n",
    "    Args:\n",
    "        image_dir: The directory containing the Yale Face images. Defaults to './photometrics_images/yaleB02/'.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - image_stack: A stack of images with the ambient image subtracted and normalized.\n",
    "            - scriptV: An array of light direction vectors corresponding to each image.\n",
    "    \"\"\"\n",
    "    \n",
    "    # define the number of images to load\n",
    "    num_images = 64\n",
    "    \n",
    "    # load the ambient image (the image without any lighting)\n",
    "    filename = os.path.join(image_dir, 'yaleB02_P00_Ambient.pgm')\n",
    "    ambient_image = cv2.imread(filename, -1)\n",
    "    \n",
    "    # get the height and width of the ambient image\n",
    "    h, w = ambient_image.shape\n",
    "\n",
    "    # import glob to find all image files in the directory\n",
    "    import glob\n",
    "    \n",
    "    # get a list of all image files that match the pattern for the yaleB02 face dataset\n",
    "    d = glob.glob(os.path.join(image_dir, 'yaleB02_P00A*.pgm'))\n",
    "    \n",
    "    # randomly select a subset of images to match the num_images count\n",
    "    import random\n",
    "    d = random.sample(d, num_images)\n",
    "    \n",
    "    # extract the base filenames from the list of image paths\n",
    "    filenames = [os.path.basename(x) for x in d]\n",
    "\n",
    "    # initialize arrays for storing angles and image data\n",
    "    ang = np.zeros([2, num_images])\n",
    "    image_stack = np.zeros([h, w, num_images])\n",
    "\n",
    "    # loop through each selected image file\n",
    "    for j in range(num_images):\n",
    "        # extract the lighting angles from the filename\n",
    "        ang[0,j], ang[1,j] = np.double(filenames[j][12:16]), np.double(filenames[j][17:20])\n",
    "        \n",
    "        # load the image and subtract the ambient image to get the actual illumination effect\n",
    "        image_stack[...,j] = cv2.imread(os.path.join(image_dir, filenames[j]), -1) - ambient_image\n",
    "\n",
    "    # calculate the light direction vectors based on the extracted angles\n",
    "    x = np.cos(np.pi*ang[1,:]/180) * np.cos(np.pi*ang[0,:]/180)\n",
    "    y = np.cos(np.pi*ang[1,:]/180) * np.sin(np.pi*ang[0,:]/180)\n",
    "    z = np.sin(np.pi*ang[1,:]/180)\n",
    "    \n",
    "    # combine the direction vectors, adjusting for normal direction\n",
    "    scriptV = [-1,-1,1] * np.array([y,z,x]).transpose(1,0)  ##eggonz COMMENT: fix (-1,-1,1) normal directiona\n",
    "\n",
    "    # convert the image stack to double precision\n",
    "    image_stack = np.double(image_stack)\n",
    "    \n",
    "    # set any negative pixel values to 0 (removing underflow)\n",
    "    image_stack[image_stack<0] = 0\n",
    "    \n",
    "    # find the minimum and maximum pixel values in the image stack\n",
    "    min_val = np.min(image_stack)\n",
    "    max_val = np.max(image_stack)\n",
    "    \n",
    "    # normalize the image stack to have values between 0 and 1, or set to zeros if min and max are equal\n",
    "    if max_val != min_val:\n",
    "        image_stack = (image_stack - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        image_stack = np.zeros(image_stack.shape)  # avoid failure when all pixel values are the same\n",
    "\n",
    "    # return the processed image stack and light directions\n",
    "    return image_stack, scriptV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:17:52.280885Z",
     "start_time": "2024-09-13T15:17:52.277992Z"
    }
   },
   "outputs": [],
   "source": [
    "def photometric_stereo_face_dataset(image_dir='./images/photometrics_images/yaleB02/', path_type='average'):\n",
    "    \"\"\"\n",
    "    Performs photometric stereo to estimate the surface albedo, normals, and height map for the Yale Face Dataset under varying illumination conditions.\n",
    "\n",
    "    Args:\n",
    "        image_dir: Directory containing the input images (default is './images/photometrics_images/yaleB02/').\n",
    "        path_type: Type of path integration to use for surface height computation (default is 'average').\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # obtain many images in a fixed view under different illumination\n",
    "    print('Loading images...\\n')\n",
    "    [image_stack, scriptV] = load_face_images(image_dir)\n",
    "    [h, w, n] = image_stack.shape\n",
    "    print('Finish loading %d images.\\n' % n)\n",
    "\n",
    "    # compute the surface albedo and normal map\n",
    "    print('Computing surface albedo and normal map...\\n')\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    albedo, normals = estimate_albedo_and_normals(image_stack, scriptV, False)\n",
    "\n",
    "    # integrability check: is (dp / dy  -  dq / dx) ^ 2 small everywhere?\n",
    "    print('Integrability checking\\n')\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    p, q, SE = check_integrability(normals)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    threshold = 0.01\n",
    "\n",
    "\n",
    "    print('Number of outliers: %d\\n' % np.sum(SE > threshold))\n",
    "    SE[SE <= threshold] = float('nan')  # for good visualization\n",
    "\n",
    "    # compute the surface height map\n",
    "    print('Computing surface height map...\\n')\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    height_map = construct_surface(p, q, path_type)\n",
    "\n",
    "    # show results\n",
    "    print('Displaying results...\\n')\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    show_results(albedo, normals, height_map, SE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you estimate the albedo and surface normal using the three path types, the cell below will run for at least 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:17:55.568052Z",
     "start_time": "2024-09-13T15:17:52.295779Z"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "image_dir = './photometrics_images/yaleB02/'\n",
    "print('Column-major PMS:' + '\\n' + '-----------------------------------', end='\\n')\n",
    "photometric_stereo_face_dataset(image_dir, path_type='column')\n",
    "print('Row-major PMS:' + '\\n' + '-----------------------------------', end='\\n')\n",
    "photometric_stereo_face_dataset(image_dir, path_type='row')\n",
    "print('Average PMS:' + '\\n' + '-----------------------------------', end='\\n')\n",
    "photometric_stereo_face_dataset(image_dir, path_type='average')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-32\"></a>\n",
    "#### <font color='#FF0000'>Question 32 (1 point)</font>\n",
    "\n",
    "Observe and discuss the results for different integration paths in the `photometric_stereo_face_dataset` function.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">For the different integration paths, the visual differences are very subtle. The column-major path seems to have the most noise in the height map, while the row-major path seems to have the least. The average path seems to be a good middle ground between the two. The differences in these height maps can be due to differences in accumulation of numerical errors and differences in summing the height values for the different paths. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-33\"></a>\n",
    "#### <font color='#FF0000'>Question 33 (2 points)</font>\n",
    "\n",
    "Discuss how the images violate the assumptions of the shape-from-shading methods. Include specific input images to illustrate your points.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Photometric stereo assumes that all input images use the same number light sources. The following images violate this assumptions by having no light source:\n",
    "\n",
    "- `yaleB02_P00A+095E+00.pgm`\n",
    "- `yaleB02_P00A-130E+20.pgm`\n",
    "- `yaleB02_P00_Ambient.pgm`\n",
    "\n",
    ">There are also images that exhibit periodic noise which can be misinterpreted as shading from the lighting. This is problematic because this information is used to determine the shape of the face. These images are:\n",
    "\n",
    "- `yaleB02_P00A+020E-10.pgm`\n",
    "- `yaleB02_P00A+025E+00.pgm`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-34\"></a>\n",
    "#### <font color='#FF0000'>Question 34 (2 points)</font>\n",
    "\n",
    "How would the results improve when the problematic images are removed? Try it out and show the results in your notebook.\n",
    "\n",
    "To complete this task, you should implement the `load_face_images_filtered` and `photometric_stereo_face_filtered` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:17:55.641623Z",
     "start_time": "2024-09-13T15:17:55.629545Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_face_images_filtered(image_dir='./photometrics_images/yaleB02/', bad_images=[]):\n",
    "    \"\"\"\n",
    "    Loads a set of face images from the Yale Face Database, excluding problematic ones, and computes corresponding light directions.\n",
    "\n",
    "    Args:\n",
    "        image_dir: The directory containing the Yale Face images. Defaults to './photometrics_images/yaleB02/'.\n",
    "        bad_images: A list of filenames to exclude from the image stack.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - image_stack: A stack of images with the ambient image subtracted, normalized, and problematic images removed.\n",
    "            - scriptV: An array of light direction vectors corresponding to each image.\n",
    "    \"\"\"\n",
    "\n",
    "    num_images = 64 - len(bad_images)\n",
    "    \n",
    "    # load the ambient image (the image without any lighting)\n",
    "    filename = os.path.join(image_dir, 'yaleB02_P00_Ambient.pgm')\n",
    "    ambient_image = cv2.imread(filename, -1)\n",
    "    \n",
    "    # get the height and width of the ambient image\n",
    "    h, w = ambient_image.shape\n",
    "\n",
    "    # import glob to find all image files in the directory\n",
    "    import glob\n",
    "    \n",
    "    # get a list of all image files that match the pattern for the yaleB02 face dataset\n",
    "    d = glob.glob(os.path.join(image_dir, 'yaleB02_P00A*.pgm'))\n",
    "\n",
    "    # remove images on the bad image list\n",
    "    bad_images = {'./photometrics_images/yaleB02/' + n for n in bad_images}\n",
    "    d = list(set(d) - bad_images)\n",
    "    \n",
    "\n",
    "    # randomly select a subset of images to match the num_images count\n",
    "    import random\n",
    "    d = random.sample(d, num_images)\n",
    "    \n",
    "    # extract the base filenames from the list of image paths\n",
    "    filenames = [os.path.basename(x) for x in d]\n",
    "\n",
    "    # initialize arrays for storing angles and image data\n",
    "    ang = np.zeros([2, num_images])\n",
    "    image_stack = np.zeros([h, w, num_images])\n",
    "\n",
    "    # loop through each selected image file\n",
    "    for j in range(num_images):\n",
    "        # extract the lighting angles from the filename\n",
    "        ang[0,j], ang[1,j] = np.double(filenames[j][12:16]), np.double(filenames[j][17:20])\n",
    "        \n",
    "        # load the image and subtract the ambient image to get the actual illumination effect\n",
    "        image_stack[...,j] = cv2.imread(os.path.join(image_dir, filenames[j]), -1) - ambient_image\n",
    "\n",
    "    # calculate the light direction vectors based on the extracted angles\n",
    "    x = np.cos(np.pi*ang[1,:]/180) * np.cos(np.pi*ang[0,:]/180)\n",
    "    y = np.cos(np.pi*ang[1,:]/180) * np.sin(np.pi*ang[0,:]/180)\n",
    "    z = np.sin(np.pi*ang[1,:]/180)\n",
    "    \n",
    "    # combine the direction vectors, adjusting for normal direction\n",
    "    scriptV = [-1,-1,1] * np.array([y,z,x]).transpose(1,0)  ##eggonz COMMENT: fix (-1,-1,1) normal directiona\n",
    "\n",
    "    # convert the image stack to double precision\n",
    "    image_stack = np.double(image_stack)\n",
    "    \n",
    "    # set any negative pixel values to 0 (removing underflow)\n",
    "    image_stack[image_stack<0] = 0\n",
    "    \n",
    "    # find the minimum and maximum pixel values in the image stack\n",
    "    min_val = np.min(image_stack)\n",
    "    max_val = np.max(image_stack)\n",
    "    \n",
    "    # normalize the image stack to have values between 0 and 1, or set to zeros if min and max are equal\n",
    "    if max_val != min_val:\n",
    "        image_stack = (image_stack - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        image_stack = np.zeros(image_stack.shape)  # avoid failure when all pixel values are the same\n",
    "\n",
    "    # return the processed image stack and light directions\n",
    "    return image_stack, scriptV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:17:55.688755Z",
     "start_time": "2024-09-13T15:17:55.670652Z"
    }
   },
   "outputs": [],
   "source": [
    "def photometric_stereo_face_filtered(bad_images=[], image_dir='./photometrics_images/yaleB02/', shadow_trick=False, path_type='average'):\n",
    "    \"\"\"\n",
    "    Performs photometric stereo to estimate the surface albedo, normals, and height map for the Yale Face Dataset, excluding problematic images.\n",
    "\n",
    "    Args:\n",
    "        bad_images: A list of filenames to exclude from the image stack (default is an empty list).\n",
    "        image_dir: Directory containing the input images (default is './photometrics_images/yaleB02/').\n",
    "        shadow_trick: Whether or not to use the shadow trick for albedo and normal estimation (default is False).\n",
    "        path_type: Type of path integration to use for surface height computation (default is 'average').\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # obtain many images in a fixed view under different illumination\n",
    "    print('Loading images...\\n')\n",
    "    [image_stack, scriptV] = load_face_images_filtered(image_dir, bad_images=bad_images)\n",
    "    [h, w, n] = image_stack.shape\n",
    "    print('Finish loading %d images.\\n' % n)\n",
    "\n",
    "    # compute the surface albedo and normal map\n",
    "    print('Computing surface albedo and normal map...\\n')\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    albedo, normals = estimate_albedo_and_normals(image_stack, scriptV, False)\n",
    "\n",
    "    # integrability check: is (dp / dy  -  dq / dx) ^ 2 small everywhere?\n",
    "    print('Integrability checking\\n')\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    p, q, SE = check_integrability(normals)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    threshold = 0.01\n",
    "\n",
    "\n",
    "    print('Number of outliers: %d\\n' % np.sum(SE > threshold))\n",
    "    SE[SE <= threshold] = float('nan')  # for good visualization\n",
    "\n",
    "    # compute the surface height map\n",
    "    print('Computing surface height map...\\n')\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    height_map = construct_surface(p, q, path_type)\n",
    "\n",
    "    # show results\n",
    "    print('Displaying results...\\n')\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    show_results(albedo, normals, height_map, SE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:17:58.985595Z",
     "start_time": "2024-09-13T15:17:55.718653Z"
    }
   },
   "outputs": [],
   "source": [
    "badfiles = ['yaleB02_P00A+095E+00.pgm', 'yaleB02_P00A-130E+20.pgm', 'yaleB02_P00A+020E-10.pgm', 'yaleB02_P00A+025E+00.pgm', 'yaleB02_P00_Ambient.pgm']\n",
    "photometric_stereo_face_filtered(badfiles, path_type='column')\n",
    "photometric_stereo_face_filtered(badfiles, path_type='row')\n",
    "photometric_stereo_face_filtered(badfiles, path_type='average')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-35\"></a>\n",
    "#### <font color='#FF0000'>Question 35 (1 point)</font>\n",
    "\n",
    "Discuss the results obtained after removing the problematic images. What are the differences in the results, and how do they compare to the original results?\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Removing the problematic images results in lower squared errors for all integration paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-36\"></a>\n",
    "#### <font color='#FF0000'>Question 36 (4 points)</font>\n",
    "\n",
    "Now, it's time to apply the algorithm to the Apple dataset using real-world 3-channel RGB inputs.\n",
    "\n",
    "For the Apple dataset, you should:\n",
    "- Load the images\n",
    "- Compute the surface albedo and normal map\n",
    "- Run the integrability check\n",
    "- Find the number of outliers\n",
    "- Compute the surface height\n",
    "- Show the results\n",
    "\n",
    "Use the images in the \"Apple\" folder. The `load_apple_images` function is provided as a helper. You should complete the `photometric_stereo_apple_dataset` function as part of this task.\n",
    "\n",
    "Observe and discuss the results for different integration paths. You may encounter difficulties using this non-synthetic dataset—try if filtering may help.\n",
    "\n",
    "Make sure to display the results using the 3 paths (column-major, row-major, and average) when constructing the surface height map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:17:59.049147Z",
     "start_time": "2024-09-13T15:17:59.039135Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_apple_images(image_dir='./photometrics_images/Apple'):\n",
    "    \"\"\"\n",
    "    Loads a set of apple images from the Apple dataset and computes corresponding light directions.\n",
    "\n",
    "    Args:\n",
    "        image_dir: The directory containing the Apple images. Defaults to './photometrics_images/Apple/'.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - image_stack: A stack of images with only the red channel extracted and normalized.\n",
    "            - scriptV: An array of light direction vectors corresponding to each image.\n",
    "    \"\"\"\n",
    "\n",
    "    # define the number of images to load\n",
    "    num_images = 99\n",
    "    \n",
    "    # load a sample image to determine its dimensions\n",
    "    filename = os.path.join(image_dir, 'I_0000.png')\n",
    "    try_image = cv2.imread(filename, -1)\n",
    "    \n",
    "    # get the height and width of the sample image\n",
    "    h, w = try_image[:,:,0].shape\n",
    "\n",
    "    # import glob to find all image files in the directory\n",
    "    import glob\n",
    "    \n",
    "    # get a list of all image files that match the pattern for the Apple dataset\n",
    "    d = glob.glob(os.path.join(image_dir, 'I_00*.png'))\n",
    "    \n",
    "    # randomly select a subset of images to match the num_images count\n",
    "    import random\n",
    "    d = random.sample(d, num_images)\n",
    "    \n",
    "    # extract the base filenames from the list of image paths and their corresponding indices\n",
    "    filenames = [os.path.basename(x) for x in d]\n",
    "    filenames_idx = [int(i.split('_')[1].split('.')[0]) for i in filenames]\n",
    "\n",
    "    # initialize arrays for storing angles and image data\n",
    "    ang = np.zeros([2, num_images])\n",
    "    image_stack = np.zeros([h, w, num_images])\n",
    "\n",
    "    # loop through each selected image file\n",
    "    for j in range(num_images):\n",
    "        # load the image and extract the red channel (index 2)\n",
    "        image_stack[...,j] = cv2.imread(os.path.join(image_dir, filenames[j]), -1)[:,:,2]\n",
    "\n",
    "    # read light direction vectors from the provided file\n",
    "    with open('./photometrics_images/Apple/light_directions_refined.txt') as file:\n",
    "        lines = [line.split() for line in file]\n",
    "        x, y, z = [], [], []\n",
    "        \n",
    "        # extract light directions corresponding to the selected images\n",
    "        for idx in filenames_idx:\n",
    "            x.append(float(lines[idx][0]))\n",
    "            y.append(float(lines[idx][1]))\n",
    "            z.append(float(lines[idx][2]))\n",
    "\n",
    "    # combine the direction vectors, adjusting for normal direction\n",
    "    scriptV = [1,-1,1] * np.array([x,y,z]).transpose(1,0)  ##eggonz COMMENT: fix (1,-1,1) normal directiona\n",
    "\n",
    "    # convert the image stack to double precision\n",
    "    image_stack = np.double(image_stack)\n",
    "    \n",
    "    # set any negative pixel values to 0 (removing underflow)\n",
    "    image_stack[image_stack < 0] = 0\n",
    "    \n",
    "    # find the minimum and maximum pixel values in the image stack\n",
    "    min_val = np.min(image_stack)\n",
    "    max_val = np.max(image_stack)\n",
    "    \n",
    "    # normalize the image stack to have values between 0 and 1, or set to zeros if min and max are equal\n",
    "    if max_val != min_val:\n",
    "        image_stack = (image_stack - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        image_stack = np.zeros(image_stack.shape)  # avoid failure when all pixel values are the same\n",
    "\n",
    "    # return the processed image stack and light directions\n",
    "    return image_stack, scriptV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:17:59.098894Z",
     "start_time": "2024-09-13T15:17:59.089541Z"
    }
   },
   "outputs": [],
   "source": [
    "def photometric_stereo_apple_dataset(image_dir='./photometrics_images/Apple/', path_type='average'):\n",
    "    \"\"\"\n",
    "    Performs photometric stereo to estimate the surface albedo, normals, and height map for the Apple Dataset under varying illumination conditions.\n",
    "\n",
    "    Args:\n",
    "        image_dir: Directory containing the input images (default is './photometrics_images/Apple/').\n",
    "        path_type: Type of path integration to use for surface height computation (default is 'average').\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # obtain many images in a fixed view under different illumination\n",
    "    print('Loading images...\\n')\n",
    "    [image_stack, scriptV] = load_apple_images(image_dir)\n",
    "    [h, w, n] = image_stack.shape\n",
    "    print('Finish loading %d images.\\n' % n)\n",
    "\n",
    "    # compute the surface albedo and normal map\n",
    "    print('Computing surface albedo and normal map...\\n')\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    albedo, normals = estimate_albedo_and_normals(image_stack, scriptV, shadow_trick=False)\n",
    "\n",
    "    # integrability check: is (dp / dy  -  dq / dx) ^ 2 small everywhere?\n",
    "    print('Integrability checking\\n')\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    p, q, SE = check_integrability(normals)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    threshold = 0.01\n",
    "\n",
    "\n",
    "    print('Number of outliers: %d\\n' % np.sum(SE > threshold))\n",
    "    SE[SE <= threshold] = float('nan')  # for good visualization\n",
    "\n",
    "    # compute the surface height map\n",
    "    print('Computing surface height map...\\n')\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    height_map = construct_surface(p, q, path_type)\n",
    "    # show results\n",
    "    print('Displaying results...\\n')\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    show_results(albedo, normals, height_map, SE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you estimate the albedo and surface normal using the three path types, the cell below will run for at least 1 minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:18:24.512388Z",
     "start_time": "2024-09-13T15:17:59.116587Z"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "image_dir = './photometrics_images/Apple/'\n",
    "print('Column-major PMS:' + '\\n' + '-----------------------------------', end='\\n')\n",
    "photometric_stereo_apple_dataset(image_dir, path_type='column')\n",
    "print('Row-major PMS:' + '\\n' + '-----------------------------------', end='\\n')\n",
    "photometric_stereo_apple_dataset(image_dir, path_type='row')\n",
    "print('Average PMS:' + '\\n' + '-----------------------------------', end='\\n')\n",
    "photometric_stereo_apple_dataset(image_dir, path_type='average')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-37\"></a>\n",
    "#### <font color='#FF0000'>Question 37 (1 point)</font>\n",
    "\n",
    "Observe and discuss the results for different integration paths in the `photometric_stereo_apple_dataset` function.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The result of the different integration paths in the apple images are also subtle. The smoothest height map is obtained by the column-major path, while the row-major path has the most noise (more troughs and curves). The average path is, once again, a good middle ground between the two. Average-path seems to distribute the errors evenly, leading to less noise in the height map. Despite producing more troughs, the row-major path does not have streaks or spikes at the edges. These means the shape of the edges of the apple is more accurately represented by the row-major path, whereas the surface is better estimated by column/average paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-x\"></a>\n",
    "### **Section X: Individual Contribution Report *(Mandatory)***\n",
    "\n",
    "Because we want each student to contribute fairly to the submitted work, we ask you to fill out the textcells below. Write down your contribution to each of the assignment components in percentages. Naturally, percentages for one particular component should add up to 100% (e.g. 30% - 30% - 40%). No further explanation has to be given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name | Contribution on Research | Contribution on Programming | Contribution on Writing |\n",
    "| -------- | ------- | ------- | ------- |\n",
    "|  | - % | - % | - % |\n",
    "|  | - % | - % | - % |\n",
    "|  | - % | - % | - % |\n",
    "|  | - % | - % | - % |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - End of Notebook -"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
